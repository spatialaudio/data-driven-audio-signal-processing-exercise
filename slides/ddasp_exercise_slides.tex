\iffalse
  \documentclass[mathserif, aspectratio=43]{intbeamer}
\else
  \documentclass[aspectratio=169]{beamer}
  \usetheme{Madrid}
  \usecolortheme{dove}  % dove, whale
  \usefonttheme{professionalfonts}
  \setbeamertemplate{page number in head/foot}[appendixframenumber] % appendix pagenumbering restart
\fi

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{subcaption}
\captionsetup[subfigure]{skip=2pt} % global setting for subfigure
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{trfsigns}
%\usepackage{gensymb}
%\usepackage{macros}
\usepackage{xcolor}
%\usepackage{enumerate}
\setbeamercovered{invisible}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{comment}

%\includecomment{plottikz}
%\excludecomment{plottikz}
\definecolor{pyplotC0}{RGB}{31,119,180}

%\newcommand{\tw}{0.73}

% ===== titlepage info =====
\title[STiASP \#24512 - Exercise]%
{Selected Topics in Audio Signal Processing\\(Data-Driven Methods in Signal Processing) \#24512}

\author[Schultz, Spors]{%
    \underline{Frank Schultz}, Sascha Spors}

\date[Winter Term 2022/23]{%\raisebox{0mm}{\includegraphics[width=4.6cm]{logo.png}}\\
  Exercise -- Winter Term 2022/23}

\institute[]{Research Group Signal Processing and Virtual Acoustics,
University of Rostock}

\begin{document}
\maketitle
%
%
%
\input{orga}
%
%
%

\begin{frame}{Topics}
\begin{itemize}
\item singular value decomposition (SVD)
  \begin{itemize}
  \item 4 subspaces of a matrix
  \item left inverse, (right inverse)
  \item projection matrices
  \item dimensionality reduction of a feature space via PCA
  \end{itemize}
\item loss functions, empirical risk functions and numerical minimzation, quality measures
\begin{itemize}
\item mean squared error for prediction
\item for binary, multinomial classification
\item gradient descent to find (a suitable) minimum
\item F-score, Rsquared, Goodness-Of-Fit Test
\end{itemize}
\item prediction models based on regression
    \begin{itemize}
    \item ordinary least squares (OLS)
    \item ridge regression
    \item SVD regression
    \end{itemize}
\item classifying models based on neural networks (NN)
  \begin{itemize}
  \item using fully connected layers (DNN)
  \item using convolutional layers (CNN)
  \end{itemize}
\end{itemize}
sketch of fields that contribute to data processing / data science...
\end{frame}
% sketch of fields that contribute to data processing / data science
%
%
%
\begin{frame}{Literature}
  check textbooks in our main library
  \begin{itemize}
    \item \href{https://find.ub.uni-rostock.de/sk830}{SK830...SK840 statistics, (generalized) linear models, regression}
    \item ST 285...ST 306 machine learning, artifical intelligence, neural networks
    \item QH 212...QH 236 statistics mainly for economy, (generalized) linear models, regression
  \end{itemize}
  textbooks that I like very much
  \begin{itemize}
    \item Kevin P. Murphy (2022): "Probabilistic Machine Learning: An Introduction", MIT Press, 1st. ed.
    \href{https://probml.github.io/pml-book/book1.html}{open source book and current draft as free pdf}
    \item \href{https://math.mit.edu/~gs/}{Gilbert Strang} (2019): "Linear Algebra and Learning from Data", Wellesley, 1st ed.
  \end{itemize}
\end{frame}

\begin{frame}{Literature}
  theory textbooks that inspired me a lot
  \begin{itemize}
    \item S. Theodoridis, Machine Learning, 2nd ed. Academic Press, 2020.
    \href{https://www.sciencedirect.com/book/9780128188033/machine-learning}{free ebook}
    \item T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning, 2nd ed. Springer, 2009.
    \href{https://hastie.su.domains/ElemStatLearn/}{free ebook}
    \item G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning with Applications in R, 2nd ed. Springer, 2021. \href{https://www.statlearning.com/}{free ebook}
    \item I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.
    \item C.C. Aggarwal, Neural Networks and Deep Learning. Springer, 2018.
    \item C.C. Aggarwal, Linear Algebra and Optimization for Machine Learning. Springer, 2020.
    \item Marc P. Deisenroth, A. Aldo Faisal, Cheng S. Ong, Mathematics for Machine Learning, Cambridge, 2020.
    \item Steven L. Brunton, J. Nathan Kutz, Data Driven Science \& Engineering, Cambridge, 2019.
  \end{itemize}
\end{frame}

\begin{frame}{Resources}
  highly recommended web resources
  \begin{itemize}
    \item MIT course 18065 \href{https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/}{Matrix Methods in Data Analysis, Signal Processing, and Machine Learning} by Gilbert Strang
    \item Steven L. Brunton, J. Nathan Kutz, Data Driven Science \& Engineering, Cambridge, 2019.
    \href{http://www.databookuw.com/databook.pdf}{free draft},
    \href{http://www.databookuw.com/}{video lectures},
    \href{https://github.com/dylewsky/Data_Driven_Science_Python_Demos}{Python tutorials}
    \item A. Gereon, Hands-On Machine Learning with SciKit \& TensorFlow, 1st/2nd ed. O'Reilly, 2017/2019.
    \href{https://github.com/ageron/handson-ml2}{Python tutorials}
    \item \href{https://playground.tensorflow.org}{A Neural Network Playground---TensorFlow}
    \item courses by Andrew Ng at \url{https://www.deeplearning.ai/} and/or \url{https://www.coursera.org/}
  \end{itemize}
\end{frame}

\begin{frame}{Textbooks on Statistics}
    ML deals with stuff that is actually known for decades (at least the linear modeling part), so if we are really
    serious about to learn it deeply, we should think over concepts on
    statistical signal processing, maximum-likelihood, Bayesian vs. frequentist
    statistics, generalized linear models, hierarchical models ... ...

    textbooks that I like very much
  \begin{itemize}
  \item L. Fahrmeir, A. Hamerle, and G. Tutz, Multivariate statistische Verfahren, 2nd ed. de Gruyter, 1996.
  \href{https://www.degruyter.com/document/doi/10.1515/9783110816020/html}{free ebook}
  \item L. Fahrmeir, T. Kneib, S. Lang, and B. D. Marx, Regression, 2nd ed. Springer, 2021.
  \item A. J. Dobson and A. G. Barnett, An Introduction to Generalized Linear Models, 4th ed. CRC Press, 2018.
  \item H. Madsen, P. Thyregod, Introduction to General and Generalized Linear Models, CRC Press, 2011.
  \item A. Agresti, Foundations of Linear and Generalized Models, Wiley, 2015.
  \end{itemize}
\end{frame}





\section{Ex01: Introduction}
\begin{frame}{Structured Development of Data-Driven Methods}
\textbf{Established Procedure}
\begin{enumerate}
\item Definition of the problem and of performance measures
\item Data preparation and feature extraction
\item Spot check potential model architectures
\item Model selection
\item Evaluation and reporting
\item Application
\end{enumerate}
\textbf{Technical Aspects}
\begin{itemize}
\item identify independent / dependent variables, prepare them
\item check for potential error measures and quality measures
\item choose model type(s), identify potential model parameters and hyper parameters
\item proper data handling with train/validate/test data sets
\item train/fit model(s) including optimized hyper parameters
\item choose best model(s), final train, final test, report quality
\end{itemize}
% sketch and explain x->model->y on blackboard
\end{frame}




\begin{frame}{Audio Toy Example for Regression and SVD}
Consider the following linear combinations
$$\bm{X} \bm{\beta} + \bm{n} = \bm{y}\qquad
\bm{X} \bm{\theta} + \bm{n} = \bm{y}\qquad
\bm{X} \bm{w} + \bm{n} = \bm{y}$$
where $\bm{\beta}=\bm{\theta} = \bm{w}$ are typical variables for the model parameter vector. Let us use $\bm{\beta}$, the lecture will utilize $\bm{\theta}$.
%
\begin{itemize}
\item $\bm{X}_{M \times N}$ matrix with $M$ audio samples for each column, $n$-th column represents the $n$-th audiotrack
\item $\bm{\beta}_{N \times 1}$ column vector of scalar values that represent a dedicated gain for each audiotrack
\item $\bm{n}_{M \times 1}$ column vector that represents a $M$-sample long noise signal added to the mixdown $\bm{X} \bm{\beta}$
\item $\bm{y}_{M \times 1}$ audio signal with $M$ samples as a result of the linear combination plus noise
\end{itemize}
%
Let us assume that i) we know $\bm{X}$ (i.e. the individual audio tracks) and $\bm{y}$ (i.e. the noise-corrupted final mixdown), ii) that we do not know the noise $\bm{n}$ and iii) that we want to estimate the 'real world' mixing gains $\bm{\theta}$
% sketch Xb=y
\end{frame}


\section{Ex02: SVD / 4 Subspaces}
\begin{frame}{Singular Value Decomposition (SVD)}
most general matrix factorization that nicely reveals the 4 subspaces of a rank $r$ matrix $\bm{A}$

fundamentally important for understanding the heart beat of linear algebra

\includegraphics[width=0.4\textwidth]{figures_web/Figure_7.8_A.png}
\includegraphics[width=0.55\textwidth]{figures_web/Figure_7.8_B.png}

\begin{footnotesize}Fig. 7.8 from Kevin P. Murphy (2022): "Probabilistic Machine Learning: An Introduction" MIT Press. \href{https://probml.github.io/pml-book/book1.html}{open source book and current draft as free pdf}\end{footnotesize}

$$\bm{A} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$$

left singular vectors\quad$\bm{U} = \mathrm{eigvec}(\bm{A}\bm{A}^\mathrm{H})$
\begin{footnotesize}order must match to the corresponding singular values\end{footnotesize}

right singular vectors $\bm{V} = \mathrm{eigvec}(\bm{A}^\mathrm{H}\bm{A})$
\begin{footnotesize}order must match to the corresponding singular values\end{footnotesize}

singular value matrix $\bm{S}$, $r$ singular values on diagonal descending order

input-related matrix $\bm{V}$ and output related matrix $\bm{U}$ are unitary....

%$$\bm{V}\bm{V}^\mathrm{H}=\bm{I},\quad\bm{V}^\mathrm{H}\bm{V}=\bm{I},\quad\bm{U}\bm{U}^\mathrm{H}=\bm{I},\quad\bm{U}^\mathrm{H}\bm{U}=\bm{I}$$

\end{frame}


\begin{frame}{Singular Value Decomposition (SVD)}
input-related matrix $\bm{V}$ and output related matrix $\bm{U}$ are unitary, i.e.

$$\bm{V}\bm{V}^\mathrm{H}=\bm{I},\quad\bm{V}^\mathrm{H}\bm{V}=\bm{I},\quad\bm{U}\bm{U}^\mathrm{H}=\bm{I},\quad\bm{U}^\mathrm{H}\bm{U}=\bm{I}$$

superposition of rank-1 matrices (outer products) because diagonal singular values
$$\bm{A} = \sum_{i=1}^{\text{rank }r} \sigma_i \bm{u}_i \bm{v}_i^\mathrm{H} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$$

SVD and the 4 subspaces of matrix $\bm{A}$ with rank $r$
\begin{center}
\includegraphics[width=0.3\textwidth]{figures_web/Strang_DiffEQ_LA_fig7_3.png}
\end{center}
\begin{footnotesize}consider Fig. 7.3 from Gilbert Strang (2014): "Differential Equations and Linear Algebra". Wellesley\end{footnotesize}

\end{frame}


\begin{frame}{Singular Value Decomposition (SVD)}
SVD and the 4 subspaces of matrix $\bm{A}$ with rank $r$
\begin{center}
\includegraphics[width=0.55\textwidth]{figures_web/Strang_DiffEQ_LA_fig7_3.png}
\end{center}
\begin{footnotesize}consider Fig. 7.3 from Gilbert Strang (2014): "Differential Equations and Linear Algebra". Wellesley\end{footnotesize}

$$R(\bm{A}) = C(\bm{A}^\mathrm{H})\perp N(\bm{A})
\qquad\qquad C(\bm{A}) \perp N(\bm{A}^\mathrm{H})$$

\end{frame}


\section{Ex03: SVD and Left Inverse Brings us to OLS Linear Regression}

\begin{frame}{Ex03: SVD and Left Inverse Brings us to OLS Linear Regression}
Objectives
\begin{itemize}
\item recap SVD $\bm{X} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$/ 4 Subspaces
\item nice SVD properties and essence
\item projecting vectors onto $\bm{U}$ and $\bm{V}$ spaces
\item for full column rank matrix $\bm{X}_{M \times N}$ we can try to bring a $\bm{y}\in \mathbb{R}^M$ back to $\bm{\beta}\in \mathbb{R}^N$ solving the inverse problem for $\bm{X} \bm{\beta} + \bm{n} = \bm{y}$...we should do this by understanding the SVD
\item we then re-invented the so called left inverse of a matrix
\item this can be used to solve an inverse problem in ordinary least squares (OLS) sense
\item we check this with geometrical considerations rather than calculus
\end{itemize}
\end{frame}




\begin{frame}{SVD Basics}
due to $\bm{V}^\mathrm{H}\bm{V}=\bm{I}$ we can re-arrange
$$\bm{X} = \bm{U} \bm{S} \bm{V}^\mathrm{H} \rightarrow
\bm{X} \bm{V} = \bm{U} \bm{S}$$
rank $r$ matrix $\bm{X}$ acting on row space $\bm{v}$ maps to corresponding column space $\bm{u}$ weighted by corresponding singular value
$$\bm{X} \bm{v}_{1:r} = \sigma_{1:r} \bm{u}_{1:r}$$
matrix acting on null space $\bm{v}$ maps to zero vector $\bm{0}_{M \times 1}$
$$\bm{X} \bm{v}_{r+1:N} = \bm{0}$$
we should make a sketch to visualisze this
\end{frame}


\begin{frame}{Subspace Properties and Characteristics of Inverse Problem}

\begin{itemize}
\item full rank, no nullspace, no left null space = square matrix = exactly solvable, typically learned in basic linear algebra course
\item full column rank, no nullspace, potentially large left nullspace = tall/thin matrix, over determined system of equations, we need the left inverse, many practical problems, statistical model fitting
\item full row rank, potentially large nullspace, no left nullspace = flat/fat matrix, under-determined system of equations, we need the right inverse
\item rank deficient, potentially large nullspace, potentially large left nullspace, meaningful information of the matrix is not stored in optimum way $\rightarrow$ data-driven learning...but before doing this stuff, we should at least learn full column rank cases properly
\end{itemize}
sketches of the 4 subspaces for these 4 cases help to get the essence how the matrix maps things forward and backward
\end{frame}


\begin{frame}{Projection into V}
tall, thin and full column rank matrix $\bm{X}_{M \times N}$ with rank $r=N$ and SVD $\bm{X} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$

we assume a made up vector $\bm{\beta} = 1.234 \bm{v}_1 + 5.678 \bm{v}_r$

we should project $\bm{\beta}$ onto the $\bm{v}$ vectors, which needs $(\bm{v}_{1:r}^\mathrm{H} \bm{\beta}) \bm{v}_{1:r} = ?$

we get $\bm{v}_{1}^\mathrm{H} \bm{\beta} = 1.234$, $\bm{v}_{2:r-1}^\mathrm{H} \bm{\beta} = 0$, $\bm{v}_{r}^\mathrm{H} \bm{\beta} = 5.678$ for the weights of the $\bm{v}$
space vectors, not suprisingly due to orthonormality of the $\bm{V}$ matrix

linear combination $(\bm{v}_{1}^\mathrm{H} \bm{\beta})\bm{v}_1 + 0 + ... + 0 + (\bm{v}_{r}^\mathrm{H} \bm{\beta})\bm{v}_r = \bm{\beta}$ yields precisely the made up vector $\bm{\beta}$ that we started with

a made up vector $\bm{\alpha}$ might have non-zero weights for all $\bm{v}$ vectors, i.e. all $\bm{v}$ vectors really contribute to the spanning job...

we should start to think always in linear combinations of SVD's $\bm{V}$ and $\bm{U}$ spaces

as always a simple sketch is helping to understand

all projections at once is written as $\bm{V}^\mathrm{H} \bm{\beta}$, cf. $\bm{X}\bm{\beta} = \bm{U} \bm{S} \bm{V}^\mathrm{H}\bm{\beta}$

\end{frame}


\begin{frame}{Left Inverse Via SVD}
we want to solve for $\hat{\bm{\beta}}$, full column rank $\bm{X}_{M \times N}$, rank $r=N$
$$\bm{X} \hat{\bm{\beta}} = \bm{y}$$
apply SVD
$$\bm{U} \bm{S} \bm{V}^\mathrm{H} \hat{\bm{\beta}} = \bm{y}$$
get weights of U space projection
$$\bm{U}^\mathrm{H} \bm{U} \bm{S} \bm{V}^\mathrm{H} \hat{\bm{\beta}} = \bm{U}^\mathrm{H}\bm{y}$$
invert all singular values and setup $\bm{S}^\dagger_{N \times M} = [\mathrm{diag}(1/\sigma_i) \quad \bm{0}]$ such that
$\bm{S}^\dagger\bm{S} = \bm{I}_{N \times N}$,
this yields
$$\bm{S}^\dagger\bm{S} \bm{V}^\mathrm{H} \hat{\bm{\beta}} = \bm{S}^\dagger\bm{U}^\mathrm{H}\bm{y}$$
let matrix $\bm{V}$ act on this vector, i.e. map as linear combination into $\bm{V}$ space
$$\bm{V} \bm{V}^\mathrm{H} \hat{\bm{\beta}} = \bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H}\bm{y}
\rightarrow \hat{\bm{\beta}} = \bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H}\bm{y}$$
we might want to show that the left inverse $\bm{X}^\dagger$ can be written equivalently as
$$\bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H} = (\bm{X}^\mathrm{H}\bm{X})^{-1} \bm{X}^\mathrm{H} = \bm{X}^\dagger$$

\end{frame}




\begin{frame}{Projection Matrices}
TBD...
\end{frame}

\begin{frame}{Left Inverse / Projections}
TBD...
\end{frame}

\begin{frame}{Least Squares / Left Inverse / Projections}
TBD...
\end{frame}

\section{Exxx: TBD}
\begin{frame}{Audio Toy Example for Music Genre Classification}
TBD...
\end{frame}




% \appendix
% \section{Appendix}
% \begin{frame}{}
%   $$\im^2=-1, f(x)=x^2$$
%   \begin{figure}
%   \captionsetup{width=.75\linewidth}
%   %\includegraphics[width=\tw\textwidth]{}
%   \caption{.}
%   \label{fig:}
%   \end{figure}
% \end{frame}
\end{document}
