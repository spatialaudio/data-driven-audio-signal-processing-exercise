\iffalse
  \documentclass[mathserif, aspectratio=43]{intbeamer}
\else
  \documentclass[aspectratio=169]{beamer}
  \usetheme{Madrid}
  \usecolortheme{dove}  % dove, whale
  \usefonttheme{professionalfonts}
  \setbeamertemplate{page number in head/foot}[appendixframenumber] % appendix pagenumbering restart
\fi

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{subcaption}
\captionsetup[subfigure]{skip=2pt} % global setting for subfigure
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{trfsigns}
%\usepackage{gensymb}
%\usepackage{macros}
\usepackage{xcolor}
%\usepackage{enumerate}
\setbeamercovered{invisible}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{comment}

%\includecomment{plottikz}
%\excludecomment{plottikz}
\definecolor{pyplotC0}{RGB}{31,119,180}

%\newcommand{\tw}{0.73}

% ===== titlepage info =====
\title[STiASP \#24512 - Exercise]%
{Selected Topics in Audio Signal Processing\\(Data-Driven Methods in Signal Processing) \#24512}

\author[Schultz, Spors]{%
    \underline{Frank Schultz}, Sascha Spors}

\date[Winter Term 2022/23]{%\raisebox{0mm}{\includegraphics[width=4.6cm]{logo.png}}\\
  Exercise -- Winter Term 2022/23}

\institute[]{Research Group Signal Processing and Virtual Acoustics,
University of Rostock}

\begin{document}
\maketitle
%
%
%
\input{orga}
%
%
%
\section{Introduction}
\begin{frame}{Topics}
\begin{itemize}
\item singular value decomposition (SVD)
  \begin{itemize}
  \item 4 subspaces of a matrix
  \item left inverse, (right inverse)
  \item projection matrices
  \item dimensionality reduction of a feature space via PCA
  \end{itemize}
\item loss functions, empirical risk functions and numerical minimzation, quality measures
\begin{itemize}
\item mean squared error for prediction
\item for binary, multinomial classification
\item gradient descent to find (a suitable) minimum
\item F-score, Rsquared, Goodness-Of-Fit Test
\end{itemize}
\item prediction models based on regression
    \begin{itemize}
    \item ordinary least squares (OLS)
    \item ridge regression
    \item SVD regression
    \end{itemize}
\item classifying models based on neural networks (NN)
  \begin{itemize}
  \item using fully connected layers (DNN)
  \item using convolutional layers (CNN)
  \end{itemize}
\end{itemize}
sketch of fields that contribute to data processing / data science...
\end{frame}
% sketch of fields that contribute to data processing / data science
%
%
%
\begin{frame}{Literature}
  check textbooks in our main library
  \begin{itemize}
    \item \href{https://find.ub.uni-rostock.de/sk830}{SK830...SK840 statistics, (generalized) linear models, regression}
    \item ST 285...ST 306 machine learning, artifical intelligence, neural networks
    \item QH 212...QH 236 statistics mainly for economy, (generalized) linear models, regression
  \end{itemize}
  textbooks that I like very much
  \begin{itemize}
    \item Kevin P. Murphy (2022): "Probabilistic Machine Learning: An Introduction", MIT Press, 1st. ed.
    \href{https://probml.github.io/pml-book/book1.html}{open source book and current draft as free pdf}
    \item \href{https://math.mit.edu/~gs/}{Gilbert Strang} (2019): "Linear Algebra and Learning from Data", Wellesley, 1st ed.
  \end{itemize}
\end{frame}

\begin{frame}{Literature}
  theory textbooks that inspired me a lot
  \begin{itemize}
    \item S. Theodoridis, Machine Learning, 2nd ed. Academic Press, 2020.
    \href{https://www.sciencedirect.com/book/9780128188033/machine-learning}{free ebook}
    \item T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning, 2nd ed. Springer, 2009.
    \href{https://hastie.su.domains/ElemStatLearn/}{free ebook}
    \item G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning with Applications in R, 2nd ed. Springer, 2021. \href{https://www.statlearning.com/}{free ebook}
    \item I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.
    \item C.C. Aggarwal, Neural Networks and Deep Learning. Springer, 2018.
    \item C.C. Aggarwal, Linear Algebra and Optimization for Machine Learning. Springer, 2020.
    \item Marc P. Deisenroth, A. Aldo Faisal, Cheng S. Ong, Mathematics for Machine Learning, Cambridge, 2020.
    \item Steven L. Brunton, J. Nathan Kutz, Data Driven Science \& Engineering, Cambridge, 2019.
  \end{itemize}
\end{frame}

\begin{frame}{Resources}
  highly recommended web resources
  \begin{itemize}
    \item MIT course 18065 \href{https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/}{Matrix Methods in Data Analysis, Signal Processing, and Machine Learning} by Gilbert Strang
    \item Steven L. Brunton, J. Nathan Kutz, Data Driven Science \& Engineering, Cambridge, 2019.
    \href{http://www.databookuw.com/databook.pdf}{free draft},
    \href{http://www.databookuw.com/}{video lectures},
    \href{https://github.com/dylewsky/Data_Driven_Science_Python_Demos}{Python tutorials}
    \item A. Gereon, Hands-On Machine Learning with SciKit \& TensorFlow, 1st/2nd ed. O'Reilly, 2017/2019.
    \href{https://github.com/ageron/handson-ml2}{Python tutorials}
    \item \href{https://playground.tensorflow.org}{A Neural Network Playground---TensorFlow}
    \item courses by Andrew Ng at \url{https://www.deeplearning.ai/} and/or \url{https://www.coursera.org/}
  \end{itemize}
\end{frame}

\begin{frame}{Textbooks on Statistics}
    ML deals with stuff that is actually known for decades (at least the linear modeling part), so if we are really
    serious about to learn it deeply, we should think over concepts on
    statistical signal processing, maximum-likelihood, Bayesian vs. frequentist
    statistics, generalized linear models, hierarchical models ... ...

    textbooks that I like very much
  \begin{itemize}
  \item L. Fahrmeir, A. Hamerle, and G. Tutz, Multivariate statistische Verfahren, 2nd ed. de Gruyter, 1996.
  \href{https://www.degruyter.com/document/doi/10.1515/9783110816020/html}{free ebook}
  \item L. Fahrmeir, T. Kneib, S. Lang, and B. D. Marx, Regression, 2nd ed. Springer, 2021.
  \item A. J. Dobson and A. G. Barnett, An Introduction to Generalized Linear Models, 4th ed. CRC Press, 2018.
  \item H. Madsen, P. Thyregod, Introduction to General and Generalized Linear Models, CRC Press, 2011.
  \item A. Agresti, Foundations of Linear and Generalized Models, Wiley, 2015.
  \end{itemize}
\end{frame}






\begin{frame}{Structured Development of Data-Driven Methods}
\textbf{Established Procedure}
\begin{enumerate}
\item Definition of the problem and of performance measures
\item Data preparation and feature extraction
\item Spot check potential model architectures
\item Model selection
\item Evaluation and reporting
\item Application
\end{enumerate}
\textbf{Technical Aspects}
\begin{itemize}
\item identify independent / dependent variables, prepare them
\item check for potential error measures and quality measures
\item choose model type(s), identify potential model parameters and hyper parameters
\item proper data handling with train/validate/test data sets
\item train/fit model(s) including optimized hyper parameters
\item choose best model(s), final train, final test, report quality
\end{itemize}
% sketch and explain x->model->y on blackboard
\end{frame}




\begin{frame}{Audio Toy Example for Regression and SVD}
Consider the following linear combinations
$$\bm{X} \bm{\beta} + \bm{n} = \bm{y}\qquad
\bm{X} \bm{\theta} + \bm{n} = \bm{y}\qquad
\bm{X} \bm{w} + \bm{n} = \bm{y}$$
where $\bm{\beta}=\bm{\theta} = \bm{w}$ are typical variables for the model parameter vector. Let us use $\bm{\beta}$, the lecture will utilize $\bm{\theta}$.
%
\begin{itemize}
\item $\bm{X}_{M \times N}$ matrix with $M$ audio samples for each column, $n$-th column represents the $n$-th audiotrack
\item $\bm{\beta}_{N \times 1}$ column vector of scalar values that represent a dedicated gain for each audiotrack
\item $\bm{n}_{M \times 1}$ column vector that represents a $M$-sample long noise signal added to the mixdown $\bm{X} \bm{\beta}$
\item $\bm{y}_{M \times 1}$ audio signal with $M$ samples as a result of the linear combination plus noise
\end{itemize}
%
Let us assume that i) we know $\bm{X}$ (i.e. the individual audio tracks) and $\bm{y}$ (i.e. the noise-corrupted final mixdown), ii) that we do not know the noise $\bm{n}$ and iii) that we want to estimate the 'real world' mixing gains $\bm{\theta}$
% sketch Xb=y
\end{frame}


\section{SVD}
\begin{frame}{Singular Value Decomposition (SVD)}
most general matrix factorization that nicely reveals the 4 subspaces of a rank $r$ matrix $\bm{A}$

fundamentally important for understanding the heart beat of linear algebra

\includegraphics[width=0.4\textwidth]{figures_web/Figure_7.8_A.png}
\includegraphics[width=0.55\textwidth]{figures_web/Figure_7.8_B.png}

\begin{footnotesize}Fig. 7.8 from Kevin P. Murphy (2022): "Probabilistic Machine Learning: An Introduction" MIT Press. \href{https://probml.github.io/pml-book/book1.html}{open source book and current draft as free pdf}\end{footnotesize}

$$\bm{A} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$$

left singular vectors\quad$\bm{U} = \mathrm{eigvec}(\bm{A}\bm{A}^\mathrm{H})$
\begin{footnotesize}order must match to the corresponding singular values\end{footnotesize}

right singular vectors $\bm{V} = \mathrm{eigvec}(\bm{A}^\mathrm{H}\bm{A})$
\begin{footnotesize}order must match to the corresponding singular values\end{footnotesize}

singular value matrix $\bm{S}$, $r$ singular values on diagonal descending order

input-related matrix $\bm{V}$ and output related matrix $\bm{U}$ are unitary....

%$$\bm{V}\bm{V}^\mathrm{H}=\bm{I},\quad\bm{V}^\mathrm{H}\bm{V}=\bm{I},\quad\bm{U}\bm{U}^\mathrm{H}=\bm{I},\quad\bm{U}^\mathrm{H}\bm{U}=\bm{I}$$

\end{frame}


\begin{frame}{Singular Value Decomposition (SVD)}
input-related matrix $\bm{V}$ and output related matrix $\bm{U}$ are unitary, i.e.

$$\bm{V}\bm{V}^\mathrm{H}=\bm{I},\quad\bm{V}^\mathrm{H}\bm{V}=\bm{I},\quad\bm{U}\bm{U}^\mathrm{H}=\bm{I},\quad\bm{U}^\mathrm{H}\bm{U}=\bm{I}$$

superposition of rank-1 matrices (outer products) because diagonal singular values
$$\bm{A} = \sum_{i=1}^{\text{rank }r} \sigma_i \bm{u}_i \bm{v}_i^\mathrm{H} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$$

SVD and the 4 subspaces of matrix $\bm{A}$ with rank $r$
\begin{center}
\includegraphics[width=0.3\textwidth]{figures_web/Strang_DiffEQ_LA_fig7_3.png}
\end{center}
\begin{footnotesize}consider Fig. 7.3 from Gilbert Strang (2014): "Differential Equations and Linear Algebra". Wellesley\end{footnotesize}

\end{frame}


\begin{frame}{Singular Value Decomposition (SVD)}
SVD and the 4 subspaces of matrix $\bm{A}$ with rank $r$
\begin{center}
\includegraphics[width=0.55\textwidth]{figures_web/Strang_DiffEQ_LA_fig7_3.png}
\end{center}
\begin{footnotesize}consider Fig. 7.3 from Gilbert Strang (2014): "Differential Equations and Linear Algebra". Wellesley\end{footnotesize}

$$R(\bm{A}) = C(\bm{A}^\mathrm{H})\perp N(\bm{A})
\qquad\qquad C(\bm{A}) \perp N(\bm{A}^\mathrm{H})$$

\end{frame}







\begin{frame}{Audio Toy Example for Music Genre Classification}
TBD...
\end{frame}




% \appendix
% \section{Appendix}
% \begin{frame}{}
%   $$\im^2=-1, f(x)=x^2$$
%   \begin{figure}
%   \captionsetup{width=.75\linewidth}
%   %\includegraphics[width=\tw\textwidth]{}
%   \caption{.}
%   \label{fig:}
%   \end{figure}
% \end{frame}
\end{document}
