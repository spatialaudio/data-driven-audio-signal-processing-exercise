\iffalse
  \documentclass[mathserif, aspectratio=43]{intbeamer}
\else
  \documentclass[aspectratio=169]{beamer}
  \definecolor{urllinkcol}{RGB}{31,119,180}
  \hypersetup{colorlinks,linkcolor=,urlcolor=urllinkcol}
  \usetheme{Madrid}
  \usecolortheme{dove}  % dove, whale
  \usefonttheme{professionalfonts}
  \setbeamertemplate{page number in head/foot}[appendixframenumber] % appendix pagenumbering restart
\fi

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{subcaption}
\captionsetup[subfigure]{skip=2pt} % global setting for subfigure
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{trfsigns}
%\usepackage{gensymb}
%\usepackage{macros}
\usepackage{xcolor}
%\usepackage{enumerate}
\setbeamercovered{invisible}
%\usepackage{tikz}
%\usetikzlibrary{calc}
\usepackage{comment}

%\includecomment{plottikz}
%\excludecomment{plottikz}
\definecolor{pyplotC0}{RGB}{31,119,180}

%\newcommand{\tw}{0.73}

% ===== titlepage info =====
\title[STiASP \#24512 - Exercise]%
{Selected Topics in Audio Signal Processing\\(Data-Driven Methods in Signal Processing) \#24512}

\author[Schultz, Spors]{%
    \underline{Frank Schultz}, Sascha Spors}

\date[Winter Term 2022/23]{%\raisebox{0mm}{\includegraphics[width=4.6cm]{logo.png}}\\
  Exercise -- Winter Term 2022/23}

\institute[]{Research Group Signal Processing and Virtual Acoustics,
University of Rostock}

\begin{document}
\maketitle
%
%
%
\input{orga}
%
%
%

\begin{frame}{Topics}
\begin{itemize}
\item singular value decomposition (SVD)
  \begin{itemize}
  \item 4 subspaces of a matrix
  \item left inverse, (right inverse)
  \item projection matrices
  \item dimensionality reduction of a feature space via PCA
  \end{itemize}
\item loss functions, empirical risk functions and numerical minimization, quality measures
\begin{itemize}
\item mean squared error for prediction
\item for binary, multinomial classification
\item gradient descent to find (a suitable) minimum
\item F-score, Rsquared, Goodness-Of-Fit Test
\end{itemize}
\item prediction models based on regression
    \begin{itemize}
    \item ordinary least squares (OLS)
    \item ridge regression
    \item SVD regression
    \end{itemize}
\item classifying models based on neural networks (NN)
  \begin{itemize}
  \item using fully connected layers (DNN)
  \item using convolutional layers (CNN)
  \end{itemize}
\end{itemize}
sketch of fields that contribute to data processing / data science...
\end{frame}
% sketch of fields that contribute to data processing / data science
%
%
%
\begin{frame}{Literature}
  check textbooks in our main library
  \begin{itemize}
    \item \href{https://find.ub.uni-rostock.de/sk830}{SK830...SK840 statistics, (generalized) linear models, regression}
    \item ST 285...ST 306 machine learning, artifical intelligence, neural networks
    \item QH 212...QH 236 statistics mainly for economy, (generalized) linear models, regression
  \end{itemize}
  textbooks that I like very much
  \begin{itemize}
    \item Kevin P. Murphy (2022): "Probabilistic Machine Learning: An Introduction", MIT Press, 1st. ed.
    \href{https://probml.github.io/pml-book/book1.html}{current draft as free pdf}
    \item \href{https://math.mit.edu/~gs/}{Gilbert Strang} (2019): "Linear Algebra and Learning from Data", Wellesley, 1st ed.
  \end{itemize}
\end{frame}

\begin{frame}{Literature}
  theory textbooks that inspired me a lot
  \begin{itemize}
    \item S. Theodoridis, Machine Learning, 2nd ed. Academic Press, 2020.
    \href{https://www.sciencedirect.com/book/9780128188033/machine-learning}{free ebook}
    \item T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning, 2nd ed. Springer, 2009.
    \href{https://hastie.su.domains/ElemStatLearn/}{free ebook}
    \item G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning with Applications in R, 2nd ed. Springer, 2021. \href{https://www.statlearning.com/}{free ebook}
    \item I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning. MIT Press, 2016.
    \item C.C. Aggarwal, Neural Networks and Deep Learning. Springer, 2018.
    \item C.C. Aggarwal, Linear Algebra and Optimization for Machine Learning. Springer, 2020. \href{https://link.springer.com/book/10.1007/978-3-030-40344-7}{free ebook}
    \item Marc P. Deisenroth, A. Aldo Faisal, Cheng S. Ong, Mathematics for Machine Learning, Cambridge, 2020. \href{https://mml-book.github.io/book/mml-book.pdf}{free ebook}
    \item Steven L. Brunton, J. Nathan Kutz, Data Driven Science \& Engineering, Cambridge, 2019. \href{http://www.databookuw.com/databook.pdf}{free ebook draft}
  \end{itemize}
\end{frame}

\begin{frame}{Resources}
  highly recommended web resources
  \begin{itemize}
    \item MIT course 18065 \href{https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/}{Matrix Methods in Data Analysis, Signal Processing, and Machine Learning} by Gilbert Strang
    \item Steven L. Brunton, J. Nathan Kutz, Data Driven Science \& Engineering, Cambridge, 2019.
    \href{http://www.databookuw.com/databook.pdf}{free ebook draft},
    \href{http://www.databookuw.com/}{video lectures},
    \href{https://github.com/dylewsky/Data_Driven_Science_Python_Demos}{Python tutorials}
    \item A. G\'{e}ron, Hands-On Machine Learning with SciKit \& TensorFlow, 1st/2nd ed. O'Reilly, 2017/2019.
    \href{https://github.com/ageron/handson-ml2}{Python tutorials}
    \item \href{https://playground.tensorflow.org}{A Neural Network Playground---TensorFlow}
    \item courses by Andrew Ng at \url{https://www.deeplearning.ai/} and/or \url{https://www.coursera.org/}
  \end{itemize}
\end{frame}

\begin{frame}{Textbooks on Statistics}
    ML deals with stuff that is actually known for decades (at least the linear modeling part), so if we are really
    serious about to learn it deeply, we should think over concepts on
    statistical signal processing, maximum-likelihood, Bayesian vs. frequentist
    statistics, generalized linear models, hierarchical models ... ...

    textbooks that I like very much
  \begin{itemize}
  \item L. Fahrmeir, A. Hamerle, and G. Tutz, Multivariate statistische Verfahren, 2nd ed. de Gruyter, 1996.
  \href{https://www.degruyter.com/document/doi/10.1515/9783110816020/html}{free ebook}
  \item L. Fahrmeir, T. Kneib, S. Lang, and B. D. Marx, Regression, 2nd ed. Springer, 2021.
  \item A. J. Dobson and A. G. Barnett, An Introduction to Generalized Linear Models, 4th ed. CRC Press, 2018.
  \item H. Madsen, P. Thyregod, Introduction to General and Generalized Linear Models, CRC Press, 2011.
  \item A. Agresti, Foundations of Linear and Generalized Models, Wiley, 2015.
  \end{itemize}
\end{frame}





\section{Ex01: Introduction}
\begin{frame}{Ex0x: Introduction}
Objectives
\begin{itemize}
\item related fields for data science / learning from data
\item model concept, un- \& supervised learning, prediction
\item idea of model parameters and hyper parameters
\item structured workflow for proper data learning
\item audio toy example that is used for linear regression and SVD demo
\end{itemize}
\end{frame}

\begin{frame}{Structured Development of Data-Driven Methods}
\textbf{Established Procedure}
\begin{enumerate}
\item Definition of the problem and of performance measures
\item Data preparation and feature extraction
\item Spot check potential model architectures
\item Model selection
\item Evaluation and reporting
\item Application
\end{enumerate}
\textbf{Technical Aspects}
\begin{itemize}
\item identify independent / dependent variables, prepare them
\item check for potential error measures and quality measures
\item choose model type(s), identify potential model parameters and hyper parameters
\item proper data handling with train/validate/test data sets
\item train/fit model(s) including optimized hyper parameters
\item choose best model(s), final train, final test, report quality
\end{itemize}
% sketch and explain x->model->y on blackboard
\end{frame}




\begin{frame}{Audio Toy Example for Linear Regression and SVD}
Consider the following linear combinations
$$\bm{X} \bm{\beta} + \bm{n} = \bm{y}\qquad
\bm{X} \bm{\theta} + \bm{n} = \bm{y}\qquad
\bm{X} \bm{w} + \bm{n} = \bm{y}$$
where $\bm{\beta}=\bm{\theta} = \bm{w}$ are typical variables for the model parameter vector. Let us use $\bm{\beta}=[\beta_0, \beta_1, \beta_2, ..., \beta_{N-1}]^\mathrm{T}$ here, the lecture will rather utilize $\bm{\theta}$.
%
\begin{itemize}
\item $\bm{X}_{M \times N}$ matrix with $M$ audio samples for each column, $n$-th column represents the $n$-th audiotrack
\item $\bm{\beta}_{N \times 1}$ column vector of scalar values that represent a dedicated gain for each audiotrack
\item $\bm{n}_{M \times 1}$ column vector that represents a $M$-sample long noise signal added to the mixdown $\bm{X} \bm{\beta}$
\item $\bm{y}_{M \times 1}$ audio signal with $M$ samples as a result of the linear combination plus noise
\end{itemize}
%
Let us assume that i) we know $\bm{X}$ (i.e. the individual audio tracks) and $\bm{y}$ (i.e. the noise-corrupted final mixdown), ii) that we do not know the noise $\bm{n}$ and iii) that we want to estimate the 'real world' mixing gains $\bm{\theta}$
% sketch Xb=y
\end{frame}


\section{Ex02: SVD / 4 Subspaces}
\begin{frame}{Ex02: SVD / 4 Subspaces}
Objectives
\begin{itemize}
\item linear regression with least squares teaser
\item subspaces example on simple matrix
\item SVD and 4 subspaces within orthonormal bases V, U
\item rank-1 matrix superposition
\end{itemize}
\end{frame}

\begin{frame}{Singular Value Decomposition (SVD)}
most general matrix factorization that nicely reveals the 4 subspaces of a rank $r$ matrix $\bm{A}$

fundamentally important for understanding the heart beat of linear algebra

\includegraphics[width=0.4\textwidth]{figures_web/Figure_7.8_A.png}
\includegraphics[width=0.55\textwidth]{figures_web/Figure_7.8_B.png}

\begin{footnotesize}Fig. 7.8 from Kevin P. Murphy (2022): "Probabilistic Machine Learning: An Introduction" MIT Press. \href{https://probml.github.io/pml-book/book1.html}{open source book and current draft as free pdf}\end{footnotesize}

$$\bm{A} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$$

left singular vectors\quad$\bm{U} = \mathrm{eigvec}(\bm{A}\bm{A}^\mathrm{H})$
\begin{footnotesize}, order must match to the corresponding singular values\end{footnotesize}

right singular vectors $\bm{V} = \mathrm{eigvec}(\bm{A}^\mathrm{H}\bm{A})$
\begin{footnotesize}, order must match to the corresponding singular values\end{footnotesize}

singular value matrix $\bm{S}$, $r$ singular values on diagonal descending order

input-related matrix $\bm{V}$ and output related matrix $\bm{U}$ are unitary....

%$$\bm{V}\bm{V}^\mathrm{H}=\bm{I},\quad\bm{V}^\mathrm{H}\bm{V}=\bm{I},\quad\bm{U}\bm{U}^\mathrm{H}=\bm{I},\quad\bm{U}^\mathrm{H}\bm{U}=\bm{I}$$

\end{frame}


\begin{frame}{Singular Value Decomposition (SVD)}
input-related matrix $\bm{V}$ and output related matrix $\bm{U}$ are unitary, i.e.

$$\bm{V}\bm{V}^\mathrm{H}=\bm{I},\quad\bm{V}^\mathrm{H}\bm{V}=\bm{I},\quad\bm{U}\bm{U}^\mathrm{H}=\bm{I},\quad\bm{U}^\mathrm{H}\bm{U}=\bm{I}$$

superposition of rank-1 matrices (outer products) because diagonal singular values
$$\bm{A} = \sum_{i=1}^{\text{rank }r} \sigma_i \bm{u}_i \bm{v}_i^\mathrm{H} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$$

SVD and the 4 subspaces of matrix $\bm{A}$ with rank $r$
\begin{center}
\includegraphics[width=0.3\textwidth]{figures_web/Strang_DiffEQ_LA_fig7_3.png}
\end{center}
\begin{footnotesize}consider Fig. 7.3 from Gilbert Strang (2014): "Differential Equations and Linear Algebra". Wellesley\end{footnotesize}

\end{frame}


\begin{frame}{Singular Value Decomposition (SVD)}
SVD and the 4 subspaces of matrix $\bm{A}$ with rank $r$
\begin{center}
\includegraphics[width=0.55\textwidth]{figures_web/Strang_DiffEQ_LA_fig7_3.png}
\end{center}
\begin{footnotesize}consider Fig. 7.3 from Gilbert Strang (2014): "Differential Equations and Linear Algebra". Wellesley\end{footnotesize}

$$R(\bm{A}) = C(\bm{A}^\mathrm{H})\perp N(\bm{A})
\qquad\qquad C(\bm{A}) \perp N(\bm{A}^\mathrm{H})$$

\end{frame}


\section{Ex03: SVD and Left Inverse Brings us to OLS Linear Regression}

\begin{frame}{Ex03: SVD and Left Inverse Brings us to OLS Linear Regression}
Objectives
\begin{itemize}
\item recap SVD $\bm{X} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$/ 4 Subspaces
\item nice SVD properties and essence
\item projecting vectors onto $\bm{U}$ and $\bm{V}$ spaces
\item for full column rank matrix $\bm{X}_{M \times N}$ we can try to bring a $\bm{y}\in \mathbb{R}^M$ back to $\bm{\beta}\in \mathbb{R}^N$ solving the inverse problem for $\bm{X} \bm{\beta} + \bm{n} = \bm{y}$...we should do this by understanding the SVD
\item we then re-invented the so called left inverse of a matrix
\item this can be used to solve an inverse problem in ordinary least squares (OLS) sense
\item we check this with geometrical considerations rather than calculus
\end{itemize}
\end{frame}




\begin{frame}{SVD Basics}
due to $\bm{V}^\mathrm{H}\bm{V}=\bm{I}$ we can re-arrange
$$\bm{X} = \bm{U} \bm{S} \bm{V}^\mathrm{H} \rightarrow
\bm{X} \bm{V} = \bm{U} \bm{S}$$
rank $r$ matrix $\bm{X}$ acting on row space $\bm{v}$ maps to corresponding column space $\bm{u}$ weighted by corresponding singular value
$$\bm{X} \bm{v}_{1:r} = \sigma_{1:r} \bm{u}_{1:r}$$
matrix acting on null space $\bm{v}$ maps to zero vector $\bm{0}_{M \times 1}$
$$\bm{X} \bm{v}_{r+1:N} = \bm{0}$$
we should make a sketch to visualisze this
\end{frame}


\begin{frame}{Subspace Properties and Characteristics of Inverse Problem}

\begin{itemize}
\item full rank, no nullspace, no left null space = square matrix = exactly solvable, typically learned in basic linear algebra course
\item full column rank, no nullspace, potentially large left nullspace = tall/thin matrix, over determined system of equations, we need the left inverse, many practical problems, statistical model fitting
\item full row rank, potentially large nullspace, no left nullspace = flat/fat matrix, under-determined system of equations, we need the right inverse
\item rank deficient, potentially large nullspace, potentially large left nullspace, meaningful information of the matrix is not stored in optimum way $\rightarrow$ data-driven learning...but before doing this stuff, we should at least learn full column rank cases properly
\end{itemize}
sketches of the 4 subspaces for these 4 cases help to get the essence how a matrix maps things forward and (potentially) backward
\end{frame}


\begin{frame}{Projection into V}
\begin{itemize}
\item
tall, thin and full column rank matrix $\bm{X}_{M \times N}$ with rank $r=N$ and SVD $\bm{X} = \bm{U} \bm{S} \bm{V}^\mathrm{H}$
\item
we assume a made up vector $\bm{\beta} = 1.234 \bm{v}_1 + 5.678 \bm{v}_r$
\item
we should project $\bm{\beta}$ onto the $\bm{v}$ vectors, which needs $(\bm{v}_{1:r}^\mathrm{H} \bm{\beta}) \bm{v}_{1:r} = ?$
\item
we get $\bm{v}_{1}^\mathrm{H} \bm{\beta} = 1.234$, $\bm{v}_{2:r-1}^\mathrm{H} \bm{\beta} = 0$, $\bm{v}_{r}^\mathrm{H} \bm{\beta} = 5.678$ for the weights of the $\bm{v}$
space vectors, not suprisingly due to orthonormality of the $\bm{V}$ matrix
\item
linear combination $(\bm{v}_{1}^\mathrm{H} \bm{\beta})\bm{v}_1 + 0 + ... + 0 + (\bm{v}_{r}^\mathrm{H} \bm{\beta})\bm{v}_r = \bm{\beta}$ yields precisely the made up vector $\bm{\beta}$ that we started with
\item
a made up vector $\bm{\alpha}$ might have non-zero weights for all $\bm{v}$ vectors, i.e. all $\bm{v}$ vectors really contribute to the spanning job...
\item
we should start to think always in linear combinations of SVD's $\bm{V}$ and $\bm{U}$ spaces
\item
as always a simple sketch is helping to understand
\item
all projections at once is written as $\bm{V}^\mathrm{H} \bm{\beta}$, cf. $\bm{X}\bm{\beta} = \bm{U} \bm{S} \bm{V}^\mathrm{H}\bm{\beta}$
\end{itemize}
\end{frame}


\begin{frame}{Left Inverse Via SVD}
we want to solve for ${\bm{\beta}}$, full column rank $\bm{X}_{M \times N}$, rank $r=N$
$$\bm{X} {\bm{\beta}} = \bm{y}$$
apply SVD
$$\bm{U} \bm{S} \bm{V}^\mathrm{H} {\bm{\beta}} = \bm{y}$$
get weights of U space projection
$$\bm{U}^\mathrm{H} \bm{U} \bm{S} \bm{V}^\mathrm{H} {\bm{\beta}} = \bm{U}^\mathrm{H}\bm{y}$$
invert all singular values and setup $\bm{S}^\dagger_{N \times M} = [\mathrm{diag}(1/\sigma_i) \quad \bm{0}]$ such that
$\bm{S}^\dagger\bm{S} = \bm{I}_{N \times N}$, apply
$$\bm{S}^\dagger\bm{S} \bm{V}^\mathrm{H} {\bm{\beta}} = \bm{S}^\dagger\bm{U}^\mathrm{H}\bm{y}$$
let matrix $\bm{V}$ act on this vector, i.e. map as linear combination into $\bm{V}$ space
$$\bm{V} \bm{V}^\mathrm{H} {\bm{\beta}} = \bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H}\bm{y}
\rightarrow \text{this yields our estimator } \hat{\bm{\beta}} = \bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H}\bm{y} = \bm{X}^\dagger \bm{y}$$
we might want to show that the left inverse $\bm{X}^\dagger$ can be written equivalently as
$$\bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H} = (\bm{X}^\mathrm{H}\bm{X})^{-1} \bm{X}^\mathrm{H} = \bm{X}^\dagger$$
\end{frame}




\begin{frame}{Projection Matrices Based on Left Inverse}
For full column rank $\bm{X}_{M \times N}$, rank $r=N$ we got the left inverse
$$\bm{V} \bm{S}^\dagger\bm{U}^\mathrm{H} = (\bm{X}^\mathrm{H}\bm{X})^{-1} \bm{X}^\mathrm{H} = \bm{X}^\dagger_{N \times M}$$
from which we can create four projection matrices into the four subspaces
\begin{itemize}
\item into row space $\bm{P}_\text{row} = \bm{X}^\dagger \bm{X} = \bm{I}_{N \times N}$ \begin{footnotesize}(i.e. the concept of a left inverse)\end{footnotesize}
\item into null space $\bm{P}_\text{null} = \bm{I} - \bm{P}_\text{row} = \bm{0}_{N \times N}$ \begin{footnotesize}(because no null space when full column rank)\end{footnotesize}
\item into column space $\bm{P}_\text{column} = (\bm{X} \bm{X}^\dagger)_{M \times M}$ \begin{footnotesize}(very often called hat matrix)\end{footnotesize}
\item into left nullspace $\bm{P}_\text{left null} = \bm{I}_{M \times M} - \bm{P}_\text{column}$
\end{itemize}

So the estimator $\hat{\bm{\beta}}$ from solving the inverse problem yields a vector living in the column space
$$\bm{X} \hat{\bm{\beta}} = \bm{X} (\bm{X}^\dagger \bm{y}) = \bm{P}_\text{column} \bm{y} = \hat{\bm{y}}$$
This means that there is a remaining part (typical called residual) $\bm{e}$ from measured $\bm{y}$
$$\bm{e} = \bm{y} - \hat{\bm{y}} = \bm{y} - \bm{P}_\text{column} \bm{y} = (\bm{I}_{M \times M} - \bm{P}_\text{column}) \bm{y}=
\bm{P}_\text{left null} \bm{y} = \bm{e}$$
which lives in the left null space. Due to the orthogonal subspaces, we know that $\hat{\bm{y}} \perp \bm{e}$

\end{frame}

\begin{frame}{Least Squares Sense Solution: Calculus vs. Graphical}
Calculus way: As usually assuming full column rank, we can consider an optimization problem, where we want to minimize
squared distance, hence the name least squares
$$\min_{\text{wrt }\bm{\beta}} ||\bm{y} - \bm{X} \bm{\beta}||_2^2$$
which needs 1st / 2nd derivatives, yielding the already found solution using the left inverse
$$\hat{\bm{\beta}} = (\bm{X}^\mathrm{H}\bm{X})^{-1} \bm{X}^\mathrm{H} \bm{y}$$
Graphical way: the shortest path to get $\bm{y}$ into the column space is by orthogonal projection, we get again $\hat{\bm{y}} \perp \bm{e}$...that's why the subspace
stuff is so nice...the $\hat{\bm{y}}$ can be mapped back to row space yielding $\hat{\bm{\beta}}$
\begin{center}
\includegraphics[width=0.23\textwidth]{figures_web/Theodoridis_ML_2nd_Fig4.3.png}
\end{center}
\begin{footnotesize}consider Fig. 4.3 from S. Theodoridis, Machine Learning, 2nd ed. Academic Press, 2020\end{footnotesize}
\end{frame}

\section{Ex04: Audio Example, Linear Regression, SVD}
\begin{frame}{Ex04: Audio Example, Linear Regression, SVD}
Objectives
\begin{itemize}
\item so far theoretical stuff
\item we apply this to a toy example using audio data...listening to stuff in vectors and matrices is fun and helps us to understand what the linear algebra is doing
\end{itemize}
\end{frame}


\section{Exxx: TBD}
\begin{frame}{Audio Toy Example for Music Genre Classification}
TBD...
\end{frame}


% \section{Ex0x: Topic}
% \begin{frame}{Ex0x: Topic}
% Objectives
% \begin{itemize}
% \item
% \end{itemize}
% \end{frame}



% \appendix
% \section{Appendix}
% \begin{frame}{}
%   $$\im^2=-1, f(x)=x^2$$
%   \begin{figure}
%   \captionsetup{width=.75\linewidth}
%   %\includegraphics[width=\tw\textwidth]{}
%   \caption{.}
%   \label{fig:}
%   \end{figure}
% \end{frame}
\end{document}
