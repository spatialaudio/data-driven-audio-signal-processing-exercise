{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e093828d",
   "metadata": {},
   "source": [
    "Sascha Spors,\n",
    "Professorship Signal Theory and Digital Signal Processing,\n",
    "Institute of Communications Engineering (INT),\n",
    "Faculty of Computer Science and Electrical Engineering (IEF),\n",
    "University of Rostock,\n",
    "Germany\n",
    "\n",
    "# Data Driven Audio Signal Processing - A Tutorial with Computational Examples\n",
    "\n",
    "Winter Semester 2022/23 (Master Course #24512)\n",
    "\n",
    "- lecture: https://github.com/spatialaudio/data-driven-audio-signal-processing-lecture\n",
    "- tutorial: https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise\n",
    "\n",
    "Feel free to contact lecturer frank.schultz@uni-rostock.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b7e45",
   "metadata": {},
   "source": [
    "# Trade-Off Between Bias^2 / Variance; Regularization by Ridge Regression\n",
    "\n",
    "- we use plain **regularized least squares** based **ridge regression** to discuss a very fundamental aspect when we learn from data, i.e. we create prediction models\n",
    "- this aspect is known as bias-variance trade-off\n",
    "- in general we can split the squared sum of (true model data - predicted model data) into three components\n",
    "$$\\text{model bias}^2 + \\text{model variance} + \\text{noise variance}$$\n",
    "- a model will never explain all variance (which is actually not wanted for a useful robust model), so certain noise variance remains\n",
    "- we can influence the model bias and model variance obviously by the choice of the model, see [bias_variance_linear_regression.ipynb](bias_variance_linear_regression.ipynb) for usage of different design/feature matrices that set up models with different complexity\n",
    "- however, we cannot at the same time have lowest model bias *and* lowest model variance to reduce the overall error for predictions\n",
    "- we therefore need to find a good compromise between bias and variance and especially we need to avoid two extremes\n",
    "    - underfit case, with typically too low model complexity yielding high bias and low variance\n",
    "    - overfit case, with typically too high model complexity yielding low bias and high variance\n",
    "\n",
    "In this notebook we use a model that is in principle capable of overfitting, i.e. it can fit some amount of the noise due to its comparably too high model complexity. One way to avoid overfitting to a certain degree is the regularization of the inverse problem. Here, we use linear regression and the most simple form of regularization\n",
    "\n",
    "$$\\min_{\\text{wrt }\\mathbf{b}} (||\\mathbf{y} - \\mathbf{X} \\mathbf{b}||_2^2 + \\alpha ||\\mathbf{b}||^2_2),$$\n",
    "\n",
    "which is known as **ridge regression** or **Tikhonov regularization** with the **hyper parameter** (regularization value) $\\alpha$.\n",
    "\n",
    "The solution can be analytically given and is well known as\n",
    "\n",
    "$$\\hat{\\mathbf{b}} = (\\mathbf{X}^\\mathrm{H} \\mathbf{X} + \\alpha \\mathbf{I})^{-1} \\mathbf{X}^\\mathrm{H} \\mathbf{y}$$\n",
    "\n",
    "and results in the left inverse solution for $\\alpha=0$, i.e. linear regression with ordinary least squares (OLS).\n",
    "\n",
    "\n",
    "We will see that\n",
    "- very **small** $\\alpha$ produces **high var**iance, but **low** squared **bias**; hence, we potentially **over**fit the model\n",
    "- very **large** $\\alpha$ produces **low** **var**iance, but **high** squared **bias**; hence, we potentially **under**fit the model\n",
    "\n",
    "We could consider the specific $\\alpha$, where the sum $\\text{model bias}^2 + \\text{model variance}$ is **minimum**, an **optimum** choice for the regularization amount.\n",
    "\n",
    "We should realize that regularization does not solve the general problem of choosing an appropriate design/matrix, i.e. an appropriate model. For example, if the true model has $f(x^2)$ and the prediction model is set up for $f(x^3)$, it will be hard to train/predict for negative $x$-values, simply because the true and the prediction model have not too much in common. So, regularization can only help a little here. We could try this as a toy example on our own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562808a4",
   "metadata": {},
   "source": [
    "Useful chapters in textbooks on bias-variance-tradeoff and ridge regression:\n",
    "- [Bishop 2006] Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, Chapter 3.2, 3.1.4\n",
    "- Sergios Theodoridis, *Machine Learning*, Academic Press, 2020, 2nd ed., Chapter 3.9, 3.8\n",
    "- Kevin P. Murphy, *Machine Learning-A Probabilistic Perspective*, MIT Press, 2012, 1st ed., Chapter 6.4.4, 7.5\n",
    "- Kevin P. Murphy, *Probabilistic Machine Learning-An Introduction*, MIT Press, 2022, Chapter 4.7.6.3, 11.3\n",
    "- Trevor Hastie, Robert Tibshirani, Jerome Friedman, *The Elements of  Statistical Learning: Data Mining, Inference, and Prediction*, Springer, 2009, 2nd ed., Chapter 2.9, 3.4\n",
    "- Gareth James, Daniela Witten, Trevor Hastie, Rob Tibshirani, *An Introduction to Statistical Learning with Applications in R*, Springer, 2021, 2nd ed., Chapter 2.2.2, 6.2.1\n",
    "- Richard O. Duda, Peter E. Hart, David G. Stork, *Pattern Classification*, Wiley, 2000, 2nd ed., Chapter 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba507c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducible outputs\n",
    "rng = np.random.default_rng(12345)  # used for data creation and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(M, split=0.8, noise_scale=2, shuffled=True):\n",
    "    x = np.linspace(0, 2*np.pi, M)  # lin increase\n",
    "    # shuffle data for simple train/test split handling using [:Ns], [Ns:]\n",
    "    if shuffled:\n",
    "        rng.shuffle(x)\n",
    "\n",
    "    # design/feature matrix of the true model\n",
    "    X = np.column_stack((np.cos(1*x),\n",
    "                         np.sin(2*x),\n",
    "                         np.cos(5*x),\n",
    "                         np.cos(6*x)))\n",
    "    # add bias/intercept column to the design/feature matrix\n",
    "    X = np.hstack((np.ones((M, 1)), X))\n",
    "    # some nice numbers for the true model parameters beta\n",
    "    beta = np.array([3, 2, 1, 1/2, 1/4])\n",
    "    # outcome of true model\n",
    "    y_true = (X @ beta)[:, None]\n",
    "    # add measurement noise\n",
    "    noise = rng.normal(loc=0, scale=noise_scale, size=(M, 1))\n",
    "    y = y_true + noise\n",
    "\n",
    "    # design/feature matrix of the prediction model\n",
    "    # we create a model that can overfit the noisy data\n",
    "    # as the feature/design matrix contains also non-matching Fourier series\n",
    "    # components, thus:\n",
    "    # true model Fourier components, same as above\n",
    "    X = np.column_stack((np.cos(1*x),\n",
    "                         np.sin(2*x),\n",
    "                         np.cos(5*x),\n",
    "                         np.cos(6*x)))\n",
    "    # additional Fourier components, that do not explain our y_true\n",
    "    # but will be sensible to the measurement noise contained in y\n",
    "    if True:\n",
    "        X = np.column_stack((X,\n",
    "                             np.cos(2*x),\n",
    "                             np.cos(3*x),\n",
    "                             np.cos(4*x),\n",
    "                             np.cos(7*x),\n",
    "                             np.cos(8*x),\n",
    "                             np.cos(9*x),\n",
    "                             np.cos(10*x),\n",
    "                             np.sin(1*x),\n",
    "                             np.sin(3*x),\n",
    "                             np.sin(4*x),\n",
    "                             np.sin(5*x),\n",
    "                             np.sin(6*x),\n",
    "                             np.sin(7*x),\n",
    "                             np.sin(8*x),\n",
    "                             np.sin(9*x),\n",
    "                             np.sin(10*x)))\n",
    "    X = np.hstack((np.ones((M, 1)), X))\n",
    "\n",
    "    # split data set into training set and test set\n",
    "    Ns = int(split*M)\n",
    "    X_trn, X_tst = X[:Ns, :], X[Ns:, :]\n",
    "    y_true_trn, y_true_tst = y_true[:Ns], y_true[Ns:]  # without noise\n",
    "    y_trn, y_tst = y[:Ns], y[Ns:]  # with measurement noise\n",
    "\n",
    "    return x, X_trn, X_tst, y_true_trn, y_true_tst, y_trn, y_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scale = 5\n",
    "\n",
    "M = 2**10  # no of rows in X = no of samples\n",
    "\n",
    "Nmodels = 2**6  # number of models to be trained\n",
    "\n",
    "alpha_min, alpha_max = -2, +2\n",
    "Nalpha = 2**6 + 1\n",
    "\n",
    "alpha_vec = np.logspace(alpha_min, alpha_max, Nalpha-1)\n",
    "alpha_vec = np.insert(alpha_vec, 0, 0)  # add 0 for no regularization\n",
    "\n",
    "x, X_trn, X_tst, y_true_trn, y_true_tst, y_trn, y_tst = create_dataset(M)\n",
    "Mtest = y_tst.shape[0]\n",
    "\n",
    "# we use capital Y to refer to output data of many models stored into matrices\n",
    "Yh_tst = np.zeros((Mtest, Nmodels, Nalpha))\n",
    "Y_true_tst = np.zeros((Mtest, Nmodels, Nalpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afb4067",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_idx in range(Nmodels):\n",
    "    x, X_trn, X_tst, y_true_trn, y_true_tst, y_trn, y_tst = create_dataset(\n",
    "        M, noise_scale=noise_scale)\n",
    "    model = OLS(y_trn, X_trn, hasconst=True)\n",
    "\n",
    "    for alpha_idx, alpha in enumerate(alpha_vec):\n",
    "        results = model.fit_regularized(\n",
    "            alpha=alpha, L1_wt=0, profile_scale=False)\n",
    "        Yh_tst[:, model_idx, alpha_idx] = results.predict(X_tst)\n",
    "        # either: true model data without noise:\n",
    "        Y_true_tst[:, model_idx, alpha_idx] = np.squeeze(y_true_tst)\n",
    "        # or: take the noisy data\n",
    "        # Y_true_tst[:, model_idx, alpha_idx] = np.squeeze(y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ccf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average prediction, i.e. mean over the L models\n",
    "# which is a numerical eval of the expectation:\n",
    "ym = np.mean(Yh_tst, axis=1)  # (3.45) in [Bishop 2006]\n",
    "ym_true = np.mean(Y_true_tst, axis=1)\n",
    "\n",
    "# get integrated squared bias (numerical eval of the expectation):\n",
    "# (3.42), (3.46) in [Bishop 2006]\n",
    "bias_squared = np.mean((ym - ym_true)**2, axis=0)\n",
    "\n",
    "# get integrated variance (numerical eval of the expectation):\n",
    "# (3.43), (3.47) in [Bishop 2006]\n",
    "variance = np.mean(\n",
    "    np.mean((Yh_tst - np.expand_dims(ym, axis=1))**2, axis=1), axis=0)\n",
    "\n",
    "# find min for bias_squared+variance\n",
    "idx = np.argmin(bias_squared+variance)\n",
    "# get specific alpha for this min\n",
    "alpha_opt = alpha_vec[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49248a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1, figsize=(8, 4))\n",
    "axs.plot(alpha_vec, bias_squared, 'C0', label=r'bias$^2$', lw=2)\n",
    "axs.plot(alpha_vec, variance, 'C1', label=r'var')\n",
    "axs.plot(alpha_vec, bias_squared+variance, 'C2', label=r'bias$^2$+var')\n",
    "\n",
    "axs.plot(alpha_opt, bias_squared[idx], 'C0o')\n",
    "axs.plot(alpha_opt, variance[idx], 'C1o')\n",
    "axs.plot(alpha_opt, bias_squared[idx] + variance[idx], 'C2o')\n",
    "\n",
    "axs.set_xscale('log')\n",
    "axs.set_yscale('log')\n",
    "axs.set_xlabel(r'regularization value $\\alpha$')\n",
    "axs.set_title(r'$\\alpha_\\mathrm{opt}$='+'{:4.3f}'.format(alpha_vec[idx]))\n",
    "axs.legend()\n",
    "axs.set_xlim(10**alpha_min, 10**alpha_max)\n",
    "axs.set_ylim(1e-2, 1e1)\n",
    "axs.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split=1 here, i.e. we realize handling: train data == test data\n",
    "# we do this to conviently show y(x) for shuffled=False data\n",
    "x, X_trn, X_tst, y_true_trn, y_true_tst, y_trn, y_tst = create_dataset(\n",
    "    M, split=1, noise_scale=noise_scale, shuffled=False)\n",
    "model = OLS(y_trn, X_trn, hasconst=True)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "fig2, axs2 = plt.subplots(1, 1, figsize=(10, 3))\n",
    "\n",
    "# model with alpha=0\n",
    "results = model.fit_regularized(alpha=0, L1_wt=0, profile_scale=False)\n",
    "yh = results.predict(X_trn)\n",
    "axs2.plot(np.arange(X_trn.shape[1]), results.params, 'C0o-',\n",
    "          label=r'$\\alpha$='+'{:4.3f}'.format(alpha_vec[0]))\n",
    "axs[0, 0].plot(x, y_trn, 'C0')\n",
    "axs[0, 0].plot(x, y_true_trn, 'C1')\n",
    "axs[0, 0].plot(x, yh, 'C3')\n",
    "axs[0, 0].set_title(r'$\\alpha$='+'{:4.3f}'.format(alpha_vec[0]))\n",
    "\n",
    "# model with alpha=0.01 (stored in alpha_vec[0])\n",
    "results = model.fit_regularized(\n",
    "    alpha=alpha_vec[0], L1_wt=0, profile_scale=False)\n",
    "yh = results.predict(X_trn)\n",
    "axs2.plot(np.arange(X_trn.shape[1]), results.params, 'C1o:',\n",
    "          label=r'$\\alpha$='+'{:4.3f}'.format(alpha_vec[1]))\n",
    "axs[0, 1].plot(x, y_trn, 'C0')\n",
    "axs[0, 1].plot(x, y_true_trn, 'C1')\n",
    "axs[0, 1].plot(x, yh, 'C3')\n",
    "axs[0, 1].set_title(r'$\\alpha$='+'{:4.3f}'.format(alpha_vec[1]))\n",
    "\n",
    "# model with optimum alpha (stored in alpha_opt)\n",
    "results = model.fit_regularized(alpha=alpha_opt, L1_wt=0, profile_scale=False)\n",
    "yh = results.predict(X_trn)\n",
    "axs2.plot(np.arange(X_trn.shape[1]), results.params, 'C2o-',\n",
    "          label=r'$\\alpha_\\mathrm{opt}$='+'{:4.3f}'.format(alpha_opt))\n",
    "axs[1, 0].plot(x, y_trn, 'C0')\n",
    "axs[1, 0].plot(x, y_true_trn, 'C1')\n",
    "axs[1, 0].plot(x, yh, 'C3')\n",
    "axs[1, 0].set_title(r'$\\alpha_\\mathrm{opt}$='+'{:4.3f}'.format(alpha_opt))\n",
    "\n",
    "# model with alpha=100 (stored in alpha_vec[-1])\n",
    "results = model.fit_regularized(\n",
    "    alpha=alpha_vec[-1], L1_wt=0, profile_scale=False)\n",
    "yh = results.predict(X_trn)\n",
    "axs2.plot(np.arange(X_trn.shape[1]), results.params, 'C3o-',\n",
    "          label=r'$\\alpha$='+'{:4.3f}'.format(alpha_vec[-1]))\n",
    "axs[1, 1].plot(x, y_trn, 'C0', label='y_train (with noise)')\n",
    "axs[1, 1].plot(x, y_true_trn, 'C1', label='y_true_train (w/o noise)')\n",
    "axs[1, 1].plot(x, yh, 'C3', label='predicted y from X_trn')\n",
    "axs[1, 1].set_title(r'$\\alpha$='+'{:4.3f}'.format(alpha_vec[-1]))\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        axs[i, j].grid(True)\n",
    "        axs[i, j].set_xlabel('x')\n",
    "        axs[i, j].set_ylabel('y')\n",
    "        axs[i, j].set_xlim(x[0], x[-1])\n",
    "        axs[i, j].set_ylim(-7, 13)\n",
    "axs[1, 1].legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "axs2.legend()\n",
    "axs2.set_xlabel(\n",
    "    r'$\\beta$ coefficient index, true model features 0...4, features >4 contribute to overfit')\n",
    "axs2.set_ylabel(r'$\\beta$ value')\n",
    "axs2.set_title('prediction model parameters')\n",
    "axs2.set_xticks(np.arange(X_trn.shape[1]))\n",
    "axs2.grid(True)\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09838a13",
   "metadata": {},
   "source": [
    "## Check best regularization value for one specific data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use very noisy y data\n",
    "# we have split=1 and shuffled=False here, as we split and shuffle manually\n",
    "# below\n",
    "# we do this because we want to concatenate and re-sort the data after\n",
    "# training / testing in order to plot the data nicely\n",
    "x, X_trn, _, y_true_trn, _, y_trn, _ = create_dataset(\n",
    "    M, split=1, noise_scale=10, shuffled=False)\n",
    "\n",
    "if False:  # we could use just the features that correspond to the true model\n",
    "    X_trn = X_trn[:, 0:5]\n",
    "\n",
    "# for shuffling data\n",
    "idx = np.arange(M)\n",
    "rng.shuffle(idx)\n",
    "# shuffle data\n",
    "x = x[idx]\n",
    "X = X_trn[idx, :]\n",
    "y_true = y_true_trn[idx]\n",
    "y = y_trn[idx]\n",
    "# split data\n",
    "split = 0.8  # 80 % go into training, 20% into test data\n",
    "Ns = int(split*M)\n",
    "X_trn, X_tst = X[:Ns, :], X[Ns:, :]\n",
    "y_true_trn, y_true_tst = y_true[:Ns], y_true[Ns:]  # without noise\n",
    "y_trn, y_tst = y[:Ns], y[Ns:]  # with measurement noise\n",
    "# set up OLS model\n",
    "model = OLS(y_trn, X_trn, hasconst=True)\n",
    "# we use capital Y to refer to output data of many models stored into matrices\n",
    "Yh_trn = np.zeros((Ns, Nalpha))\n",
    "Yh_tst = np.zeros((M-Ns, Nalpha))\n",
    "# train/predict for different reguarization\n",
    "for alpha_idx, alpha in enumerate(alpha_vec):\n",
    "    results = model.fit_regularized(alpha=alpha, L1_wt=0, profile_scale=False)\n",
    "    Yh_trn[:, alpha_idx] = results.predict(X_trn)\n",
    "    Yh_tst[:, alpha_idx] = results.predict(X_tst)\n",
    "# residual check for train / test data compared with true! data (which we dont have in pratice)\n",
    "SSE_trn = np.sum((Yh_trn - y_true_trn)**2, axis=0)\n",
    "SSE_tst = np.sum((Yh_tst - y_true_tst)**2, axis=0)\n",
    "# get optimum alpha where smallest SSE_trn+SSE_tst is obtained\n",
    "alpha_opt_idx = np.argmin(SSE_trn+SSE_tst)\n",
    "alpha_opt = alpha_vec[alpha_opt_idx]\n",
    "print(alpha_opt)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(alpha_vec, SSE_trn, 'C0', label='sum squared errors train data')\n",
    "plt.plot(alpha_vec, SSE_tst, 'C2', label='sum squared errors test data')\n",
    "plt.plot(alpha_vec, SSE_trn+SSE_tst, 'C3', label='SSE train + SSE test')\n",
    "plt.plot(alpha_vec[alpha_opt_idx], SSE_trn[alpha_opt_idx]+SSE_tst[alpha_opt_idx], 'C3o')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlim(10**alpha_min, 10**alpha_max)\n",
    "plt.xlabel(r'regularization value $\\alpha$')\n",
    "plt.title(r'$\\alpha_\\mathrm{opt}$='+'{:4.3f}'.format(alpha_opt))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedd52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test data\n",
    "# re-shuffle them, so bring it into original order, such that x is increasing\n",
    "y_true = np.concatenate((y_true_trn, y_true_tst))\n",
    "y_true = y_true[np.argsort(idx)]\n",
    "y = np.concatenate((y_trn, y_tst))\n",
    "y = y[np.argsort(idx)]\n",
    "yh = np.concatenate((Yh_trn[:, alpha_opt_idx], Yh_tst[:, alpha_opt_idx]))\n",
    "yh = yh[np.argsort(idx)]\n",
    "# plot y(x)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(x[np.argsort(idx)], y, 'C0', label='measured y (with noise)')\n",
    "plt.plot(x[np.argsort(idx)], y_true, 'C1', label='true y (w/o noise)')\n",
    "plt.plot(x[np.argsort(idx)], yh, 'C3', label='predicted y')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.xlim(x[np.argsort(idx)][0], x[np.argsort(idx)][-1])\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301efb9",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "- the notebooks are provided as [Open Educational Resources](https://en.wikipedia.org/wiki/Open_educational_resources)\n",
    "- the text is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    "- the code of the IPython examples is licensed under the [MIT license](https://opensource.org/licenses/MIT)\n",
    "- feel free to use the notebooks for your own purposes\n",
    "- please attribute the work as follows: *Frank Schultz, Data Driven Audio Signal Processing - A Tutorial Featuring Computational Examples, University of Rostock* ideally with relevant file(s), github URL https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise, commit number and/or version tag, year."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myddasp",
   "language": "python",
   "name": "myddasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
