{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489c7bcd-d5b9-4a10-beb9-e58120f4758b",
   "metadata": {},
   "source": [
    "Sascha Spors,\n",
    "Professorship Signal Theory and Digital Signal Processing,\n",
    "Institute of Communications Engineering (INT),\n",
    "Faculty of Computer Science and Electrical Engineering (IEF),\n",
    "University of Rostock,\n",
    "Germany\n",
    "\n",
    "# Data Driven Audio Signal Processing - A Tutorial with Computational Examples\n",
    "\n",
    "Winter Semester 2022/23 (Master Course #24512)\n",
    "\n",
    "- lecture: https://github.com/spatialaudio/data-driven-audio-signal-processing-lecture\n",
    "- tutorial: https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise\n",
    "\n",
    "Feel free to contact lecturer frank.schultz@uni-rostock.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201797c2-3cc5-4cce-b1b9-684286f3b759",
   "metadata": {},
   "source": [
    "# Exercise 8: Model Complexity Bias / Variance Problem\n",
    "\n",
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import lstsq\n",
    "# a nice homework: add Ridge regression using\n",
    "from sklearn.linear_model import Ridge\n",
    "# and check what this is doing on our simple examples\n",
    "\n",
    "# machine learning routine shown with the\n",
    "# simple linear model X theta = y\n",
    "# no gradient descent is used here, see exercise 09 \n",
    "\n",
    "\n",
    "def train(X, y_meas, print_theta):\n",
    "    theta, res, rank, singval = lstsq(X, y_meas)\n",
    "    print('number of theta coeff:' , theta.shape)\n",
    "    if print_theta:\n",
    "        print('theta', theta)\n",
    "        #print(res)\n",
    "        #print('rank', rank)\n",
    "        #print('singval', singval)\n",
    "    return theta\n",
    "\n",
    "\n",
    "def predict(X, theta):\n",
    "    y_pred = X @ theta  # e.g. == theta[0] * X[:,0] + theta[1] * X[:,1]\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def get_Rsq(y_meas, y_pred):\n",
    "    ym = np.mean(y_meas)\n",
    "    #print('overall variance of data') \n",
    "    #print(np.dot(y_meas - ym, y_meas - ym))\n",
    "    #print('which is a superposition of residual stuff (not explained by our model) and the explained variance')\n",
    "    #print(np.dot(y_meas - y_pred, y_meas - y_pred) + np.dot(y_pred - ym, y_pred - ym))\n",
    "    # from that we can derive so called R^2\n",
    "\n",
    "    # fraction of explained variance by regression vs. overall variance\n",
    "    Rsq = np.dot(y_pred - ym, y_pred - ym) / np.dot(y_meas - ym, y_meas - ym)\n",
    "    #print('Rsquared', Rsq)\n",
    "    # can also be derived with (see lecture slides) \n",
    "    TSS = np.sum((y_meas - ym)**2)  # this is variance of y_meas\n",
    "    RSS = np.sum((y_meas - y_pred)**2)  # sum of squares of residuals == res from lstsq()\n",
    "    # print(np.allclose(RSS, res))\n",
    "    Rsq = 1 - RSS/TSS\n",
    "    #print('Rsquared', Rsq)\n",
    "    return Rsq\n",
    "\n",
    "\n",
    "def plot(X1, y_meas, y_pred, Rsq, title_str):\n",
    "    plt.figure()\n",
    "    plt.plot(X1,y_meas, label='measured', lw=3)\n",
    "    plt.plot(X1, y_pred, label='predicted')\n",
    "    plt.plot(X1, y_ideal_model, 'gold', label='ideal model')\n",
    "    plt.xlabel('X[:,1]')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title_str+r', $R^2=$'+str(Rsq))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    \n",
    "def check_model(X, y_train, y_test, print_theta=True):\n",
    "    # training\n",
    "    theta = train(X, y_train, print_theta)  #fit model\n",
    "    y_pred = predict(X, theta)\n",
    "    Rsq = get_Rsq(y_train, y_pred)\n",
    "    plot(X1, y_train, y_pred, Rsq, 'training data')\n",
    "    # test\n",
    "    y_pred = predict(X, theta)\n",
    "    Rsq = get_Rsq(y_test, y_pred)\n",
    "    plot(X1, y_test, y_pred, Rsq, 'test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7454da-c3dd-4be5-accc-898593e8ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "N = 2**12  # number of data points\n",
    "xmax = 3.\n",
    "X1 = np.arange(-N//2, +N//2)*2./N*xmax\n",
    "y_ideal_model = 1 + 3*X1**3  + 20*np.cos(2*np.pi*X1)\n",
    "\n",
    "# training data\n",
    "mean, stdev = 0, 3\n",
    "noise = np.squeeze(rng.normal(mean, stdev, [N, 1]))\n",
    "y_train = 1 + 3*X1**3  + 20*np.cos(2*np.pi*X1) + 1*noise\n",
    "\n",
    "# test data, slightly different than training\n",
    "# 'good' models should act robust on this data\n",
    "mean, stdev = 0, 4\n",
    "noise = np.squeeze(rng.normal(mean, stdev, [N, 1]))\n",
    "pct = 1.075  # we achieve R^2 = 0.9 for no-noise data with best models\n",
    "y_test = 1*pct + 3*pct * X1**3 + 20*pct * np.cos(2*pct * np.pi*X1) + 0*noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247d789b-e621-43c8-904b-912576495058",
   "metadata": {},
   "source": [
    "### check models of different complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb4b1f-c011-4bf7-9f1c-bf377c31e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear univariate regression\n",
    "X = np.array([np.ones(N), X1]).T  # y = intercept * ones(N) + slope * X1 => y = n + m x \n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676f550-15cf-475b-aafa-d83fcb3f47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression using x^2 as feature\n",
    "X = np.array([np.ones(N), X1**2]).T\n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3d4959-8a69-498d-a81e-0397628f0fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know what y_meas is composed of -> use this information\n",
    "# for practical data we don't have this ground truth :-(\n",
    "X = np.array([np.ones(N), X1**3, np.cos(2*np.pi*X1)]).T\n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb0a89-a20d-4e61-92fe-596089a510cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we model only the x^3 part\n",
    "X = np.array([np.ones(N), X1**3]).T\n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b98ba-fb09-4fb6-af98-b4deb01286c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we model only the cosine part\n",
    "# which is a very poor model\n",
    "X = np.array([np.ones(N), np.cos(2*np.pi*X1)]).T\n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f54eae-febe-4608-b574-19516caf976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use polynomial up to x^3, no cosine\n",
    "X = np.array([np.ones(N), X1**1, X1**2, X1**3]).T\n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c209a-cfb6-4989-8c37-9891784b080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can utilize Vandermonde matrix to get same model\n",
    "# X = np.array([np.ones(N), X1**1, X1**2, X1**3]).T ==\n",
    "pol_order = 3\n",
    "X = np.vander(X1, pol_order+1, increasing=True)\n",
    "check_model(X, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc11265-3619-4e09-b3e4-69259e02a066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many polynomials plus the exact cosine part\n",
    "pol_order = 20\n",
    "X = np.concatenate((np.vander(X1, pol_order+1, increasing=True),\n",
    "                    np.expand_dims(np.cos(2*np.pi*X1), axis=1)),\n",
    "                   axis=1)\n",
    "check_model(X, y_train, y_test, print_theta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7e420-21ab-4cad-8e0c-1db0881de70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we model a fourier series with number of coeff == data points - 1\n",
    "# this leads to almost exact fit onto data\n",
    "# we never ever do this when we want to learn from data such that\n",
    "# we can make robust predictions on unseen data\n",
    "# number of model coeff << model input data\n",
    "# otherwise we just 'store' the data in the coefficients\n",
    "tmp = np.expand_dims(X1/xmax*np.pi, axis=1) * np.arange(1, 1+2**11-1, 1)\n",
    "X = np.concatenate((np.ones([N,1]),  # for intercept\n",
    "                    np.cos(tmp),\n",
    "                    np.sin(tmp)), axis=1)\n",
    "print(N, 'data points vs.', X.shape[1], 'model coefficients')\n",
    "check_model(X, y_train, y_test, print_theta=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2afbe-858a-483e-bc13-d7c04a28cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we model a fourier series with many coeff\n",
    "# this yields fair model performance but actually\n",
    "# other models above that use x^3 and cos as regressors\n",
    "# are better because:\n",
    "# - less theta coeff\n",
    "# - higher or about the same R^2\n",
    "# - more robust training vs. test data\n",
    "tmp = np.expand_dims(X1/xmax*np.pi, axis=1) * np.arange(1, 7, 1)\n",
    "X = np.concatenate((np.ones([N,1]),  # for intercept\n",
    "                    np.cos(tmp),\n",
    "                    np.sin(tmp)), axis=1)\n",
    "print(N, 'data points vs.', X.shape[1], 'model coefficients')\n",
    "check_model(X, y_train, y_test, print_theta=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96bff94-6ef3-40e3-8273-2aa218d2edb7",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "- the notebooks are provided as [Open Educational Resources](https://en.wikipedia.org/wiki/Open_educational_resources)\n",
    "- the text is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    "- the code of the IPython examples is licensed under the [MIT license](https://opensource.org/licenses/MIT)\n",
    "- feel free to use the notebooks for your own purposes\n",
    "- please attribute the work as follows: *Frank Schultz, Data Driven Audio Signal Processing - A Tutorial Featuring Computational Examples, University of Rostock* ideally with relevant file(s), github URL https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise, commit number and/or version tag, year."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1743232f157dd0954c61aae30535e75a2972519a625c7e796bafe0cd9a07bf7e"
  },
  "kernelspec": {
   "display_name": "myddasp",
   "language": "python",
   "name": "myddasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
