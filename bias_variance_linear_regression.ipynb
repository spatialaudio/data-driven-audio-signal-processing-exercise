{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219d803a",
   "metadata": {},
   "source": [
    "Sascha Spors,\n",
    "Professorship Signal Theory and Digital Signal Processing,\n",
    "Institute of Communications Engineering (INT),\n",
    "Faculty of Computer Science and Electrical Engineering (IEF),\n",
    "University of Rostock,\n",
    "Germany\n",
    "\n",
    "# Data Driven Audio Signal Processing - A Tutorial with Computational Examples\n",
    "\n",
    "Winter Semester 2022/23 (Master Course #24512)\n",
    "\n",
    "- lecture: https://github.com/spatialaudio/data-driven-audio-signal-processing-lecture\n",
    "- tutorial: https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise\n",
    "\n",
    "Feel free to contact lecturer frank.schultz@uni-rostock.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e84b71",
   "metadata": {},
   "source": [
    "# Trade-Off Between Bias^2 / Variance and Model Complexity for Linear Regression\n",
    "\n",
    "- we use plain ordinary **least squares** (OLS) based **linear regression**\n",
    "- we check **over**-/**underfitting** via bias$^2$/variance on models that were trained and predicted on noisy data (note here: **train data=test data**)\n",
    "- for this toy example we know the real world (unnoisy) data, because we know the linear model equation that creates these data, so we are pretty sure about our interpretations on the models' performances\n",
    "- in reality we deal with an unknown model, so we should be pretty cautious on the model complexity and over-/underfitting cases\n",
    "- a robust model has a good **trade-off of bias^2/variance** (see this notebook) and predicts reasonable outcomes to unknown input data (this is part of another notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fe3d6",
   "metadata": {},
   "source": [
    "Useful chapters in textbooks:\n",
    "- [Bishop 2006] Christopher M. Bishop, Pattern Recognition and Machine Learning, Springer, 2006, Chapter 3.2\n",
    "- Kevin P. Murphy, Machine Learning-A Probabilistic Perspective, MIT Press, 2012, 1st ed., Chapter 6.4.4\n",
    "- Kevin P. Murphy, Probabilistic Machine Learning-An Introduction, MIT Press, 2022, Chapter 4.7.6.3\n",
    "- Gareth James, Daniela Witten, Trevor Hastie, Rob Tibshirani, An Introduction to Statistical Learning with Applications in R, Springer, 2nd ed., 2021, Chapter 2.2.2\n",
    "- Trevor Hastie, Robert Tibshirani, Jerome Friedman, The Elements of  Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2nd ed., 2009, Chapter 2.9\n",
    "- Richard O. Duda, Peter E. Hart, David G. Stork, Pattern Classification, Wiley, 2000, 2nd ed., Chapter 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d969206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa15502",
   "metadata": {},
   "source": [
    "## Real World Model and Real World Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations / samples:\n",
    "N = 2**8\n",
    "# real world model with x as input variable to create 4 features:\n",
    "x = np.linspace(0, 2*np.pi, N)\n",
    "X = np.column_stack((np.cos(x),\n",
    "                     np.sin(2*x),\n",
    "                     np.cos(5*x),\n",
    "                     np.cos(6*x)))\n",
    "# add a bias/intercept column to the design/feature matrix:\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "hasconst = True\n",
    "# some nice numbers for the true model parameters beta:\n",
    "beta = np.array([3, 2, 1, 1/2, 1/4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22514278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'real world' data with the design matrix of 'real world' model\n",
    "y = np.dot(X, beta)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(y, 'k-')\n",
    "plt.xlabel(\"independent features' input variable x\")\n",
    "plt.ylabel(('dependent variable yn'))\n",
    "plt.title('real world data as linear model (x -> 4 features + intercept)')\n",
    "plt.xlim(0, N)\n",
    "plt.ylim(-2, 8)\n",
    "plt.grid(True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb69fb",
   "metadata": {},
   "source": [
    "## Function for Train / Predict and Calc Bias^2 / Variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeca815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_of_model(X, noise_scale=0.5):\n",
    "    # add bias column to the design matrix\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    hasconst = True\n",
    "    print('\\nshape of model/feature matrix X:',\n",
    "          X.shape,\n",
    "          '\\nrank of matrix X / # of model parameters:',\n",
    "          np.linalg.matrix_rank(X))\n",
    "    # init random number generator to reproduce results\n",
    "    rng = np.random.default_rng(1234)\n",
    "    # generate L data sets with added noise\n",
    "    L = 2**7\n",
    "    noise = rng.normal(loc=0, scale=noise_scale, size=(N, L))\n",
    "    Yn = y[:, None] + noise\n",
    "    # alloc memory for all predictions\n",
    "    Yhat = np.zeros((N, L))\n",
    "    # train and predict L models on these L data sets\n",
    "    for i in range(L):\n",
    "        model = OLS(Yn[:, i], X, hasconst=hasconst)  # OLS model\n",
    "        results = model.fit()  # train the model\n",
    "        Yhat[:, i] = results.predict(X)  # predict outcome\n",
    "    \n",
    "    # get average prediction, i.e. mean over the L models\n",
    "    # which is a numerical eval of the expectation:\n",
    "    ym = np.mean(Yhat, axis=1)  # (3.45) in [Bishop 2006]\n",
    "    \n",
    "    # get integrated squared bias (numerical eval of the expectation):\n",
    "    # note: y is the real world data\n",
    "    bias_squared = np.mean((ym - y)**2)  # (3.42), (3.46) in [Bishop 2006]\n",
    "    \n",
    "    # get integrated variance (numerical eval of the expectation):\n",
    "    variance = np.mean(\n",
    "        np.mean((Yhat - ym[:, None])**2, axis=1),\n",
    "        axis=0)  # (3.43), (3.47) in [Bishop 2006]\n",
    "\n",
    "    for i in range(L):\n",
    "        axs[0].plot(Yn[:, i])\n",
    "        axs[1].plot(Yhat[:, i])\n",
    "\n",
    "    axs[1].plot(y, 'k-', label='true model')\n",
    "    \n",
    "    axs[1].plot(np.mean(Yhat, axis=1), ':', color='gold', label='$\\mu(\\hat{Y})$')\n",
    "    \n",
    "    \n",
    "    axs[1].plot(np.mean(Yhat, axis=1) + np.std(Yhat, axis=1), '--', lw=0.75,\n",
    "                color='gold', label='$\\mu(\\hat{Y}) + \\sigma(\\hat{Y})$')\n",
    "    axs[1].plot(np.mean(Yhat, axis=1) - np.std(Yhat, axis=1), '-.', lw=0.75,\n",
    "                color='gold', label='$\\mu(\\hat{Y}) - \\sigma(\\hat{Y})$')\n",
    "    \n",
    "    \n",
    "    axs[1].set_title(r'bias$^2$='+'{:4.3f}'.format(\n",
    "        bias_squared)+', var='+'{:4.3f}'.format(\n",
    "        variance)+r', bias$^2$+var='+'{:4.3f}'.format(\n",
    "        bias_squared+variance))\n",
    "    for i in range(2):\n",
    "        axs[i].set_xlim(0, N)\n",
    "        axs[i].set_ylim(-2, 8)\n",
    "        axs[i].grid(True)\n",
    "        axs[i].set_xlabel(\"independent features' input variable x\")\n",
    "    axs[0].set_ylabel('dependent variable yn = outcome')\n",
    "    axs[1].set_ylabel('predicted variable yhat')\n",
    "    axs[1].legend()\n",
    "    print('bias^2 + variance  = ', bias_squared+variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaefad6",
   "metadata": {},
   "source": [
    "## Check Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1edf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.copy(x)[:, None]\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "bias_variance_of_model(X)\n",
    "axs[0].set_title('underfit, too low model complexity, high bias, low var');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfedd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.cos(x), np.sin(x)))\n",
    "# <=N//2 makes sure we do not use more model parameters than signal samples\n",
    "# in order to solve this as a least-squares problem, i.e. using left-inverse\n",
    "for m in range(2, N//2):\n",
    "    X = np.column_stack((X, np.sin(m*x), np.cos(m*x)))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "bias_variance_of_model(X)\n",
    "axs[0].set_title('overfit, too high model complexity, low bias, high var');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b84657",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.cos(x),\n",
    "                     np.sin(2*x),\n",
    "                     np.cos(5*x),\n",
    "                     np.cos(6*x)))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "bias_variance_of_model(X)  # lowest possible bias^2+variance, because we\n",
    "# know the true model (which in practice never happens!)\n",
    "# the remaining variance is from the added noise\n",
    "axs[0].set_title(\n",
    "    'true model, lowest bias, lowest var');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.column_stack((np.cos(x),\n",
    "                     np.sin(2*x)))\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 3))\n",
    "bias_variance_of_model(X)\n",
    "axs[0].set_title('good bias/var trade-off for unknown true model');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fb954",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "- the notebooks are provided as [Open Educational Resources](https://en.wikipedia.org/wiki/Open_educational_resources)\n",
    "- feel free to use the notebooks for your own purposes\n",
    "- the text is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    "- the code of the IPython examples is licensed under under the [MIT license](https://opensource.org/licenses/MIT)\n",
    "- please attribute the work as follows: *Frank Schultz, Data Driven Audio Signal Processing - A Tutorial Featuring Computational Examples, University of Rostock* ideally with relevant file(s), github URL https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise, commit number and/or version tag, year."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myddasp",
   "language": "python",
   "name": "myddasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
