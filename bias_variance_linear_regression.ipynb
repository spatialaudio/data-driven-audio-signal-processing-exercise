{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219d803a",
   "metadata": {},
   "source": [
    "Sascha Spors,\n",
    "Professorship Signal Theory and Digital Signal Processing,\n",
    "Institute of Communications Engineering (INT),\n",
    "Faculty of Computer Science and Electrical Engineering (IEF),\n",
    "University of Rostock,\n",
    "Germany\n",
    "\n",
    "# Data Driven Audio Signal Processing - A Tutorial with Computational Examples\n",
    "\n",
    "Winter Semester 2022/23 (Master Course #24512)\n",
    "\n",
    "- lecture: https://github.com/spatialaudio/data-driven-audio-signal-processing-lecture\n",
    "- tutorial: https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise\n",
    "\n",
    "Feel free to contact lecturer frank.schultz@uni-rostock.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e84b71",
   "metadata": {},
   "source": [
    "# Trade-Off Between Bias^2 / Variance and Check for Model Complexity\n",
    "\n",
    "- we use plain ordinary **least squares** (OLS) based **linear regression** to discuss a very fundamental aspect when we learn from data, i.e. we create prediction models\n",
    "- this aspect is known as bias-variance trade-off\n",
    "- in general we can split the squared sum of (true model data - predicted model data) into three components\n",
    "$$\\text{model bias}^2 + \\text{model variance} + \\text{noise variance}$$\n",
    "- a model will never explain all variance (which is actually not wanted for a useful robust model), so certain noise variance remains\n",
    "- we can influence the model bias and model variance obviously by the choice of the model  \n",
    "- however, we cannot at the same time have lowest model bias *and* lowest model variance to reduce the overall error for predictions\n",
    "- we therefore need to find a good compromise between bias and variance and especially we need to avoid two extremes\n",
    "    - underfit case, with typically too low model complexity yielding high bias and low variance\n",
    "    - overfit case, with typically too high model complexity yielding low bias and high variance\n",
    "\n",
    "In this notebook we therefore check **over**-/**underfitting** via bias$^2$/variance quantities and $R_{\\text{adjusted}}^2$ on models that were trained and predicted on noisy data (note here: **train data=test data**).\n",
    "\n",
    "For this toy example we know the true world (unnoisy) data, because we know the linear model equation that creates these data; hence, we are pretty sure about our interpretations on the performances of the different models.\n",
    "In real practice we deal with an unknown model equation, so we should properly check for over-/underfitting on our model estimates.\n",
    "\n",
    "A robust prediction model should have a **reasonable trade-off between bias^2/variance** and reasonable **high** $R_{\\text{adjusted}}^2$ **mean** but **low** $R_{\\text{adjusted}}^2$ **variance** (see this notebook).\n",
    "\n",
    "A robust prediction model should predict **reasonable outcomes to unknown input data**, such that it **generalizes well** on **unseen data**. This is part of another notebook, see [bias_variance_ridge_regression.ipynb](bias_variance_ridge_regression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fe3d6",
   "metadata": {},
   "source": [
    "Useful chapters in textbooks on this fundamental aspect:\n",
    "- [Bishop 2006] Christopher M. Bishop, *Pattern Recognition and Machine Learning*, Springer, 2006, Chapter 3.2\n",
    "- Sergios Theodoridis, *Machine Learning*, Academic Press, 2020, 2nd ed., Chapter 3.9\n",
    "- Kevin P. Murphy, *Machine Learning-A Probabilistic Perspective*, MIT Press, 2012, 1st ed., Chapter 6.4.4\n",
    "- Kevin P. Murphy, *Probabilistic Machine Learning-An Introduction*, MIT Press, 2022, Chapter 4.7.6.3\n",
    "- Trevor Hastie, Robert Tibshirani, Jerome Friedman, *The Elements of  Statistical Learning: Data Mining, Inference, and Prediction*, Springer, 2009, 2nd ed., Chapter 2.9\n",
    "- Gareth James, Daniela Witten, Trevor Hastie, Rob Tibshirani, *An Introduction to Statistical Learning with Applications in R*, Springer, 2021, 2nd ed., Chapter 2.2.2\n",
    "- Richard O. Duda, Peter E. Hart, David G. Stork, *Pattern Classification*, Wiley, 2000, 2nd ed., Chapter 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d969206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.api import OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa15502",
   "metadata": {},
   "source": [
    "## True Model and Its Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations / samples:\n",
    "N = 2**8\n",
    "# true model with x as input variable to create 4 features:\n",
    "x = np.linspace(0, 2*np.pi, N)\n",
    "X = np.column_stack((np.cos(x),\n",
    "                     np.sin(2*x),\n",
    "                     np.cos(5*x),\n",
    "                     np.cos(6*x)))\n",
    "# add a bias/intercept column to the design/feature matrix:\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "hasconst = True\n",
    "# some nice numbers for the true model parameters beta:\n",
    "beta = np.array([3, 2, 1, 1/2, 1/4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22514278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'true' data with the design matrix of 'true' model\n",
    "y = np.dot(X, beta)\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.plot(y, 'k-')\n",
    "plt.xlabel(\"independent features' input variable x\")\n",
    "plt.ylabel(('dependent variable yn'))\n",
    "plt.title('true model data as linear model (x -> 4 features + intercept)')\n",
    "plt.xlim(0, N)\n",
    "plt.ylim(-2, 8)\n",
    "plt.grid(True)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb69fb",
   "metadata": {},
   "source": [
    "## Function for Train / Predict and Calc Bias^2 / Variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeca815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variance_of_model(X, noise_scale=1/3):\n",
    "    # add bias column to the design matrix\n",
    "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    hasconst = True\n",
    "    print('\\nshape of model/feature matrix X:',\n",
    "          X.shape,\n",
    "          '\\nrank of matrix X / # of model parameters:',\n",
    "          np.linalg.matrix_rank(X))\n",
    "    # init random number generator to reproduce results\n",
    "    rng = np.random.default_rng(12345)\n",
    "    # generate L data sets with added noise\n",
    "    L = 2**7\n",
    "    noise = rng.normal(loc=0, scale=noise_scale, size=(N, L))\n",
    "    Yn = y[:, None] + noise\n",
    "    # alloc memory for all predictions\n",
    "    Yhat = np.zeros((N, L))\n",
    "    rsquared_adj = np.zeros(L)\n",
    "    # train and predict L models on these L data sets\n",
    "    for i in range(L):\n",
    "        model = OLS(Yn[:, i], X, hasconst=hasconst)  # OLS model\n",
    "        results = model.fit()  # train the model\n",
    "        Yhat[:, i] = results.predict(X)  # predict outcome\n",
    "        rsquared_adj[i] = results.rsquared_adj\n",
    "\n",
    "    # get average prediction, i.e. mean over the L models\n",
    "    # which is a numerical eval of the expectation:\n",
    "    ym = np.mean(Yhat, axis=1)  # (3.45) in [Bishop 2006]\n",
    "\n",
    "    # get integrated squared bias (numerical eval of the expectation):\n",
    "    # note: y is the true model data\n",
    "    bias_squared = np.mean((ym - y)**2)  # (3.42), (3.46) in [Bishop 2006]\n",
    "\n",
    "    # get integrated variance (numerical eval of the expectation):\n",
    "    variance = np.mean(\n",
    "        np.mean((Yhat - ym[:, None])**2, axis=1),\n",
    "        axis=0)  # (3.43), (3.47) in [Bishop 2006]\n",
    "\n",
    "    for i in range(L):\n",
    "        axs[0, 0].plot(Yn[:, i])\n",
    "        axs[0, 1].plot(Yhat[:, i])\n",
    "\n",
    "    axs[0, 1].plot(y, 'k-', label='true model')\n",
    "\n",
    "    axs[0, 1].plot(np.mean(Yhat, axis=1), ':',\n",
    "                   color='gold', label='$\\mu(\\hat{Y})$')\n",
    "\n",
    "    axs[0, 1].plot(np.mean(Yhat, axis=1) + np.std(Yhat, axis=1), '--', lw=0.75,\n",
    "                   color='gold', label='$\\mu(\\hat{Y}) \\pm \\sigma(\\hat{Y})$')\n",
    "    axs[0, 1].plot(np.mean(Yhat, axis=1) - np.std(Yhat, axis=1), '-.', lw=0.75,\n",
    "                   color='gold')\n",
    "\n",
    "    axs[0, 1].set_title(r'bias$^2$='+'{:4.3f}'.format(\n",
    "        bias_squared)+', var='+'{:4.3f}'.format(\n",
    "        variance)+r', bias$^2$+var='+'{:4.3f}'.format(\n",
    "        bias_squared+variance))\n",
    "    for i in range(2):\n",
    "        axs[0, i].set_xlim(0, N)\n",
    "        axs[0, i].set_ylim(-2, 8)\n",
    "        axs[0, i].grid(True)\n",
    "        axs[0, i].set_xlabel(\"independent features' input variable x\")\n",
    "    axs[0, 0].set_ylabel('dependent variable yn')\n",
    "    axs[0, 1].set_ylabel('predicted variable yhat')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    axs[1, 0].plot(rsquared_adj)\n",
    "    axs[1, 0].set_title(r'$\\hat{\\mu}(R_{adj}^2)='+'{:4.3f}$'.format(\n",
    "        np.mean(rsquared_adj))+r', $\\hat{\\sigma}(R_{adj}^2)='+'{:4.3f}$'.format(\n",
    "        np.std(rsquared_adj)))\n",
    "    axs[1, 0].set_xlim(0, L)\n",
    "    axs[1, 0].set_ylim(0, 1)\n",
    "    axs[1, 0].set_xlabel('model index')\n",
    "    axs[1, 0].set_ylabel(r'$R_{adj}^2$')\n",
    "    axs[1, 0].grid(True)\n",
    "\n",
    "    axs[1, 1].set_xlabel('intentionally empty')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print('bias^2 + variance  = ', bias_squared+variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaefad6",
   "metadata": {},
   "source": [
    "## Check Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1edf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take just a simple line equation model y = beta1 x + beta0 here\n",
    "# note  that intercept is only added in function bias_variance_of_model(X)\n",
    "X = np.copy(x)[:, None]\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "bias_variance_of_model(X)\n",
    "axs[0, 0].set_title('underfit, too low model complexity, high bias, low var');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfedd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take a Fourier series expansion model here\n",
    "X = np.column_stack((np.cos(x), np.sin(x)))  # init with first two features\n",
    "# add more features according to a Fourier series expansion\n",
    "# <=N//2 makes sure we do not use more model parameters than signal samples\n",
    "# in order to solve this as a least-squares problem, i.e. using left-inverse\n",
    "for m in range(2, N//2):\n",
    "    X = np.column_stack((X, np.sin(m*x), np.cos(m*x)))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "bias_variance_of_model(X)\n",
    "axs[0, 0].set_title('overfit, too high model complexity, low bias, high var');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b84657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take all features of the true model here\n",
    "# (we generally not know this exactly in practice)\n",
    "X = np.column_stack((np.cos(x),\n",
    "                     np.sin(2*x),\n",
    "                     np.cos(5*x),\n",
    "                     np.cos(6*x)))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "bias_variance_of_model(X)  # lowest possible bias^2+variance, because we\n",
    "# know the true model (again: which in practice likely never will occur)\n",
    "# the remaining variance is from the added noise\n",
    "axs[0, 0].set_title('true model features, lowest bias, lowest var');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take only the first two features of the true model\n",
    "# as these oscillations explain much of the dependent variable y\n",
    "X = np.column_stack((np.cos(x),\n",
    "                     np.sin(2*x)))\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "bias_variance_of_model(X)\n",
    "axs[0, 0].set_title('reasonable bias/var trade-off if true model is unknown');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fb954",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "- the notebooks are provided as [Open Educational Resources](https://en.wikipedia.org/wiki/Open_educational_resources)\n",
    "- the text is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    "- the code of the IPython examples is licensed under the [MIT license](https://opensource.org/licenses/MIT)\n",
    "- feel free to use the notebooks for your own purposes\n",
    "- please attribute the work as follows: *Frank Schultz, Data Driven Audio Signal Processing - A Tutorial Featuring Computational Examples, University of Rostock* ideally with relevant file(s), github URL https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise, commit number and/or version tag, year."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myddasp",
   "language": "python",
   "name": "myddasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
