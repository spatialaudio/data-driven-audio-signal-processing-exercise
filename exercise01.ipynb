{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sascha Spors,\n",
    "Professorship Signal Theory and Digital Signal Processing,\n",
    "Institute of Communications Engineering (INT),\n",
    "Faculty of Computer Science and Electrical Engineering (IEF),\n",
    "University of Rostock,\n",
    "Germany\n",
    "\n",
    "# Data Driven Audio Signal Processing - A Tutorial with Computational Examples\n",
    "\n",
    "Winter Semester 2021/22 (Master Course #24512)\n",
    "\n",
    "- lecture: https://github.com/spatialaudio/data-driven-audio-signal-processing-lecture\n",
    "- tutorial: https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise\n",
    "\n",
    "Feel free to contact lecturer frank.schultz@uni-rostock.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Introduction to DDASP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planned Schedule for DDASP\n",
    "\n",
    "01. 21.10.: Introduction\n",
    "02. 28.10.: STFT\n",
    "03. 04.11.: SVD 1\n",
    "04. 11.11.: SVD 2\n",
    "05. 18.11.: Regression vs. Clustering I\n",
    "06. 25.11.: Regression vs. Clustering II\n",
    "07. 02.12.: Opimization in Machine Learning / Gradient Descent\n",
    "08. 09.12.: Model Parameters\n",
    "09. 16.12.: DNN 1\n",
    "10. 06.01.: DNN 2\n",
    "11. 13.01.: CNN 1\n",
    "12. 20.01.: CNN 2\n",
    "13. 27.01.: Autoencoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Objective\n",
    "\n",
    "- for engineers **understanding the essence** of a concept is more important than strict math proof\n",
    "    - as engineers we can leave proofs to mathematicians\n",
    "    - *example*: understanding the 4 matrix subspaces and the matrix (pseudo)-inverse based on the SVD is essential, proofs on this fundamental topic is nice to have\n",
    "- understand building blocks of machine learning for audio data processing\n",
    "- create simple tool chains from these building blocks\n",
    "- create simple applications from these tool chains\n",
    "- get an impression about real industrial applications\n",
    "- get in touch with scientific literature\n",
    "    - where to find, how to read\n",
    "    - here we will find latest tool chain inventions (if published at all, a lot of stuff is either unavailable due to company secrets, or only patent specifications exist, which usually omit heavy math)\n",
    "    - interpretation of results\n",
    "    - reproducibility\n",
    "    - re-invent tool chain\n",
    "- get in touch with major software libraries (in Python), see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Engineering Practice\n",
    "\n",
    "- engineering is about creating tools (using existing tools)\n",
    "- models are tools and thus perfectly fit to the engineering community, so nothing new building models \n",
    "- better know your tools in very detail\n",
    "- responsibility, ethics, moral\n",
    "- check our task\n",
    "    - critical reflection (higher good vs. earning money)\n",
    "    - do we really need machine support here\n",
    "    - if so, how can machines support us here, how do humans solve this task\n",
    "    - what do machines better here than humans and vice versa\n",
    "    - what is our expectation of the model perfomance\n",
    "    - handcrafted model vs. machine learned model (problem: model transparency)\n",
    "- ...\n",
    "\n",
    "## Established Procedure\n",
    "for structured development of data-driven methods (cf. the lecture)\n",
    "\n",
    "1. Definition of problem and performance measures\n",
    "2. Data preparation and feature extraction\n",
    "3. Spot check potential model architectures\n",
    "4. Model selection\n",
    "5. Evaluation and reporting\n",
    "6. Application\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Python Packages\n",
    "\n",
    "- `numpy` for matrix/tensor algebra\n",
    "- `scipy` for important science math stuff\n",
    "- `pandas` for data handling\n",
    "- `scikit-learn` for predictive data analysis, machine learning\n",
    "- `matplotlib` for plotting\n",
    "- `tensorflow` deep learning with DNNs, CNNs...\n",
    "- `pytorch` deep learning with DNNs, CNNs...audio handling\n",
    "- `librosa`+`ffmpeg` music/audio analysis + en-/decoding/stream support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications for Machine Learning in Audio\n",
    "\n",
    "Some examples for applications are given below. Nowadays industrial applications use a combination of different ML techniques to provide an intended consumer service. \n",
    "\n",
    "- supervised learning (mostly prediction by clustering / regression)\n",
    "    - query by humming\n",
    "    - music/genre recognition & recommendation\n",
    "    - speech recognition\n",
    "    - disease prediction by breathing sound analysis\n",
    "    - acoustic surveillance of machines (keypad noise to text?!)\n",
    "    - gun shot / alert sound detection\n",
    "    - beam forming / direction of arrival (DOA)\n",
    "    - composing (cf. Beethoven Symphony Nr. 10)\n",
    "    - deep audio fakes (human-made vs. machine-made replica)\n",
    "    - Auto EQ (mix should sound as reference mix?!)\n",
    "- unsupervised learning (mostly clustering, dimensionality reduction)\n",
    "    - noise reduction\n",
    "    - echo cancellation\n",
    "    - feedback cancellation\n",
    "    - speech / language recognition\n",
    "    - compression\n",
    "    - feature creation (typical spectrum of pop music, classical...)\n",
    "    - feature calculation (perceived loudness, cf. replay gain adaption) \n",
    "    - key recognition\n",
    "- reinforcement learning\n",
    "    - human tasks: how to compose a hit single, how to mix a hit single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for Student Projects\n",
    "\n",
    "- song recognition\n",
    "- key recognition\n",
    "- chord recognition\n",
    "- de-noising\n",
    "- genre classification and recommendation service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digital Audio Signal Formats / Parameters\n",
    "\n",
    "- for computer processing **digital** signals are required\n",
    "- analog -> analog-to-digital converter -> digital\n",
    "- typically represented as streams/files\n",
    "    - uncompressed as e.g. PCM\n",
    "    - lossless compressed as e.g. FLAC\n",
    "    - lossy compressed as e.g. AC3 / MP3 / AAC / Vorbis / G711 / G722\n",
    "- encoder\n",
    "    - typically large processing load, since mostly off-line rendering or some latency allowed\n",
    "    - if lossy compression very often a psycho-acoustical model is employed to reduce data\n",
    "- decoder\n",
    "    - typically very low processing load due to low computational capabilities and real time demand\n",
    "- most DSP algorithms work on uncompressed audio data, thus inherent parameters are:\n",
    "    - bit resolution / quantization (typically in the range of 8-24 Bit integer, 32/64 Bit floating)\n",
    "    - sampling frequency (typical: 1/2/4 x 32, 44.1, 48 kHz, sometimes also 8/12/16 kHz)\n",
    "    - number of audio channels (mono, stereo, 5.1, 7.1, 4-128 in microphone arrays, several hundreds in audio productions)\n",
    "- vector / matrix representation\n",
    "\n",
    "### Quantization\n",
    "\n",
    "- uncompressed audio typically uses [Pulse Code Modulation](https://en.wikipedia.org/wiki/Pulse-code_modulation) (PCM) to represent the data digitally\n",
    "- quantization of amplitude values is required\n",
    "- typically done with linear quantizer\n",
    "- number of bits $B$\n",
    "- then there are $2^B$ possible quantization steps, e.g. for $B=8$ this leads to $256$ quantization steps\n",
    "- when assigning **integer** numbers to sample values the convention holds (for the **midtread quantizer**)\n",
    "    - minimum integer is $-(2^{B-1})$, e.g. for $B=8$ this leads to integer -128\n",
    "    - maximum integer is $+(2^{B-1})-1$, e.g. for $B=8$ this leads to integer +127\n",
    "    - zero can be exactly represented\n",
    "    - sample values smaller than $-(2^{B-1})$ / larger than $+(2^{B-1})-1$ will be clipped to the min/max integer\n",
    "- convention for analog-to-digital converters (ADC), digital-to-analog converters (DAC) and audio files is to interpret the samples for the range -1...+1\n",
    "- unless using explicitly a fixed point DSP (still often used for embedded hardware, less power consumption, smaller chip size), nowadays (PC based) processing is performed with floating/double precision\n",
    "- thus scaling the (integer) data might be required, cf. `scale_wav()` below\n",
    "\n",
    "### Read and Plot PCM Wav File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def scale_wav(x):\n",
    "    print('dtype for wav file content:', x.dtype)\n",
    "    if x.dtype == 'float32' or x.dtype == 'float64':\n",
    "        return(x)  # already in +-1.0 double range\n",
    "    else:  # we assume integer coded wav files:\n",
    "        tmp = str(x.dtype)\n",
    "        print('quantization bit resolution might be lower than storage bit resolution!')\n",
    "        # normalize to bring to +-1.0 double range\n",
    "        a = 1/(2**(int(tmp[3:])-1))\n",
    "        return(a*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'audio_ex01/'\n",
    "\n",
    "# fs, x = wavfile.read(folder+'sine1k_16Bit.wav')  # integer PCM\n",
    "fs, x = wavfile.read(folder+'sine1k_24Bit.wav')  # integer PCM\n",
    "# fs, x = wavfile.read(folder+'sine1k_32Bit.wav')  # float PCM\n",
    "# fs, x = wavfile.read(folder+'sine1k_64Bit.wav')  # double PCM\n",
    "\n",
    "x = scale_wav(x)\n",
    "# to work with x should have double precision unless\n",
    "# special applications require for another format\n",
    "print('dtype for read in signal x:', x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.stem(x[:48], basefmt='C0:', linefmt='C0:', markerfmt='C0o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('x[k]')\n",
    "\n",
    "# if required the samples can additionally be normalized\n",
    "# for example to represent analog units, such as sound\n",
    "# pressure if the audio signal was recorded by a\n",
    "# calibrated microphone\n",
    "# furthermore we can plot over the time rather than sample\n",
    "# index\n",
    "# example:\n",
    "k = np.arange(x.size)\n",
    "pressure_norm = 20  # full scale represents 20 Pascal peak\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.stem(k[:48]/fs*1000, x[:48] * pressure_norm,\n",
    "         basefmt='C0:', linefmt='C0:', markerfmt='C0o')\n",
    "plt.xlabel('t in ms')\n",
    "plt.ylabel('x in Pascal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copyright\n",
    "\n",
    "- the notebooks are provided as [Open Educational Resources](https://en.wikipedia.org/wiki/Open_educational_resources)\n",
    "- feel free to use the notebooks for your own purposes\n",
    "- the text is licensed under [Creative Commons Attribution 4.0](https://creativecommons.org/licenses/by/4.0/)\n",
    "- the code of the IPython examples is licensed under under the [MIT license](https://opensource.org/licenses/MIT)\n",
    "- please attribute the work as follows: *Frank Schultz, Data Driven Audio Signal Processing - A Tutorial Featuring Computational Examples, University of Rostock* ideally with relevant file(s), github URL https://github.com/spatialaudio/data-driven-audio-signal-processing-exercise, commit number and/or version tag, year.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1743232f157dd0954c61aae30535e75a2972519a625c7e796bafe0cd9a07bf7e"
  },
  "kernelspec": {
   "display_name": "myddasp",
   "language": "python",
   "name": "myddasp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
